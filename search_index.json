[["index.html", "An Introduction to R for Economics and Business Research Prerequisites", " An Introduction to R for Economics and Business Research Julian Langer Lachlan Deer Ulrich Bergmann 2021-09-27 Prerequisites If you want to follow along with the code in the book, you will need access to some of the data sets we use. You can download a zip archive of all the data here. Extract the contents of the zip archive and place them in a subdirectory data. Then the code should work as is. "],["basics-of-r.html", "Chapter 1 Basics of R 1.1 The Workspace 1.2 Vectors 1.3 Checking cases with conditionals 1.4 Functions 1.5 Iterations Sources", " Chapter 1 Basics of R Before we can dive into the actual work with data you should be familiar with the basics of the R programming language. We will not spend too much time here as you have already seen many of the concepts in our Python sequence and they will only look slightly different in R. 1.1 The Workspace 1.1.1 Working directory To know in which folder we are working, you can use the getwd function. getwd() ## [1] &quot;/Users/runner/work/pp4rs-rstats/pp4rs-rstats&quot; If you want to set a specific working directory, you can do this using the setwd function. We will not follow this approach as there is a better one by now. #setwd(&quot;/Users/jlanger/Dropbox/uzh_programming/r_programming&quot;) Instead of setting the working directory explicitly, we create a new R project. In order to do so, we click on File &gt; New Project and create a new project folder in a directory of our choice. 1.1.2 R projects This new folder now includes an .Rproj file. If we click on this file a new RStudio session is started, our environment/workspace is cleaned, and our working directory is automatically set to the project folder. Notice also that our command history and all open editor windows for the project are restored once we open it. 1.1.3 Removing and adding objects from workspace Let’s create two variables. y = 1 x = 2 To see which objects are available in our workspace, we can use the ls function. ls() ## [1] &quot;x&quot; &quot;y&quot; If you want to remove an object, use the rm function. rm(y) ls() ## [1] &quot;x&quot; If you want to remove all object to start with a clean slate, type the following. rm(list = ls()) ls() ## character(0) 1.1.4 Installing and loading packages Additional functionalities are available in R packages. To install these you have to use the install.packages function. The package name has to be passed as a string. # you do neet the repos argument! # install.packages(&quot;purrr&quot;, repos = &quot;http://cran.us.r-project.org&quot;) To then load a package, we use the library function. Here, we will load a packages with the curious name purrr that we’ll use later. library(purrr) 1.2 Vectors Vectors provide the basic organization of data in R. There are basically two types of vectors: atomic vectors – logical, numeric, character – which are homogenous, lists, which can be heterogenous and can contain other lists. Let’s start with the atomic vectors. 1.2.1 Atomic vector types 1.2.1.1 Logical Logical vectors (or booleans) can have three different values in R: TRUE ## [1] TRUE FALSE ## [1] FALSE NA ## [1] NA The first two you already know from Python. What about the third one? It is R’s way of saying something is not available. For example, assume you have missing data in your dataframe and you conduct a logical comparison such as whether a value is bigger than 3. If a value is missing, then the resulting comparison for that row cannot be TRUE or FALSE but NA. By the way, to see the type of a vector, you can use the typeof method. typeof(TRUE) ## [1] &quot;logical&quot; 1.2.1.2 Numeric Next in line are the numeric vectors. These come in two flavors: integers and doubles. typeof(2.0) ## [1] &quot;double&quot; typeof(2) ## [1] &quot;double&quot; As you can see, R stores every number as a double by default. This takes up a lot of memory so if you are sure you only need integers you can append the number with a ‘L’ to force the coercion to integer values. typeof(2L) ## [1] &quot;integer&quot; Finally, note that while integers only have one type of NA value, doubles also have the values Inf and -Inf. You can test for missing values or infinite values by using the following functions: is.finite() is.infinite() is.na() 1.2.1.3 Character Apart from the logical and numeric vectors there exist character vectors which allow you to store strings. These are created by using single or double quotes. hal = &quot;I&#39;m sorry, Dave. I&#39;m afraid I can&#39;t do that.&quot; hal ## [1] &quot;I&#39;m sorry, Dave. I&#39;m afraid I can&#39;t do that.&quot; hal2 = &#39;I\\&#39;m sorry, Dave. I\\&#39;m afraid I can\\&#39;t do that.&#39; hal2 ## [1] &quot;I&#39;m sorry, Dave. I&#39;m afraid I can&#39;t do that.&quot; 1.2.2 Vector coercion Just like Python, R features implicit as well as explicit coercion. You can do explicit coercion by using the following functions: as.logical() as.integer() as.double() as.character() I will now give you some examples of implicit coercion: If you pass a logical vector to a function that expects numeric vectors, it converts FALSE to 0 and TRUE and 1. TRUE + FALSE ## [1] 1 You can also go implicitly from numerical to logical vectors. People sometimes use numeric vectors for logical conditions: x = 0 y = 2 if (x) { print(&#39;Hello&#39;) } if (y) { print(&#39;World&#39;) } ## [1] &quot;World&quot; As you can see here, 0 gets converted into FALSE while every other value gets converted to TRUE. To check whether a vector is of a specific type, use one of the following functions from the purrr package: is_logical() is_integer() is_double() is_numeric() is_character() is_atomic() is_list() is_vector() 1.2.3 Vectors with multiple values So far we have only spoken if vectors with length 1. However, vectors can store several elements of the same type. You can create such vectors using the c() function. Let’s check out strings first. my_strings = c(&#39;This&#39;, &#39;is&#39;, &#39;a&#39;, &#39;vector.&#39;) my_strings ## [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;vector.&quot; length(my_strings) ## [1] 4 # strings do not get automatically combined if you use the print fuction print(my_strings) ## [1] &quot;This&quot; &quot;is&quot; &quot;a&quot; &quot;vector.&quot; # you need to combine the strings first with a function such as paste() print(paste(my_strings, collapse = &#39; &#39;)) ## [1] &quot;This is a vector.&quot; The numeric vectors come next. my_numbers_1 = c(1, 2, 3, 4) my_numbers_1 ## [1] 1 2 3 4 # a way of creating a numeric sequence my_numbers_2 = 1:4 my_numbers_2 ## [1] 1 2 3 4 # another way with seq() my_numbers_3 = seq(1, 10, 2) my_numbers_3 ## [1] 1 3 5 7 9 If you create a vector with values of different type, R will automatically fall back to the most comprehensive type. c(TRUE, 1) ## [1] 1 1 c(TRUE, 1, &#39;Hello&#39;) ## [1] &quot;TRUE&quot; &quot;1&quot; &quot;Hello&quot; To check the length of a vector, use the length function. Note that when you apply to a character vector it still gives you the number of elements, not the number of characters. length(c(&#39;Hallo&#39;)) ## [1] 1 length(c(1, 2, 3)) ## [1] 3 1.2.4 Operating with vectors and recycling Notice that you can do the usual arithmetic operations with vectors. Addition, multiplication etc. with scalars works as you would probaly expect. c(1, 2, 3) * 2 ## [1] 2 4 6 c(1, 2, 3) + 2 ## [1] 3 4 5 If you operate with two vectors of the same length, the operator is applied element-wise. c(1, 2, 3) + c(1, 2, 3) ## [1] 2 4 6 c(1, 2, 3) * c(1, 2, 3) ## [1] 1 4 9 What happens though if you multiply two vectors of different length? c(1, 2, 3, 4) * c(1, 2) ## [1] 1 4 3 8 Since the second vector is shorter than the first one, it gets recycled, i.e. it gets copied until it has the same length as the first one. c(1, 2, 3, 4) * c(1, 2, 1, 2) ## [1] 1 4 3 8 Note that R does not warn you about this as long as the longer vector is a multiple of the shorter one. c(1, 2, 3, 4) * c(1, 2, 3) ## Warning in c(1, 2, 3, 4) * c(1, 2, 3): longer object length is not a multiple of ## shorter object length ## [1] 1 4 9 4 1.2.5 Names If you want to name the elements of a vector, you can do so using base R or purrr. # name elements during creation, note that you do not need quotes my_named_vector = c(a = 1, b = 2, c = 3, d = 4) my_named_vector ## a b c d ## 1 2 3 4 # use purrr to set the names after creation my_named_vector = set_names(1:4, c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;)) my_named_vector ## a b c d ## 1 2 3 4 1.2.6 Subsetting There are basically three ways of subsetting a vector with the subsetting function []. You can pass a numeric vector containing the indices of the elements you are interested in. a = c(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;) a[c(1, 4)] ## [1] &quot;A&quot; &quot;D&quot; # negative indices for dropping elements a[c(-1)] ## [1] &quot;B&quot; &quot;C&quot; &quot;D&quot; # duplicate elements by using the same index again a[c(1, 1, 1)] ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; You can pass a boolean vector to select the elements which meet a certain criterion. a = 1:10 a &lt; 5 ## [1] TRUE TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE a[a &lt; 5] ## [1] 1 2 3 4 You can pass the names of elements as a character vector if you work with a named vector. a = c(a = 1, b = 2, c = 3, d = 4) a[c(&#39;b&#39;, &#39;d&#39;)] ## b d ## 2 4 1.2.7 Lists Lists differ from atomic vectors in that they can be heterogenous, i.e. store vectors or elements of different types and are recursive in that they can contain other lists. 1.2.7.1 Defining lists To define a list, use the list function. To analyze its structure, you can apply the str function to it. # list with 4 scalar elements my_list = list(1, 2, 3, 4) str(my_list) ## List of 4 ## $ : num 1 ## $ : num 2 ## $ : num 3 ## $ : num 4 # list with 1 column-vector element my_list = list(c(1, 2, 3, 4)) str(my_list) ## List of 1 ## $ : num [1:4] 1 2 3 4 # list with a boolean, a number, and a string element my_list = list(1, TRUE, &#39;Hello, hello&#39;) str(my_list) ## List of 3 ## $ : num 1 ## $ : logi TRUE ## $ : chr &quot;Hello, hello&quot; # list of two lists my_list = list(list(1, 2), list(&#39;a&#39;, &#39;b&#39;)) str(my_list) ## List of 2 ## $ :List of 2 ## ..$ : num 1 ## ..$ : num 2 ## $ :List of 2 ## ..$ : chr &quot;a&quot; ## ..$ : chr &quot;b&quot; 1.2.7.2 Subsetting lists Before we learn about subsetting, let’s create a fancy list which features sublists, names and heterogeneity. fancy_list = list(a = 1:12, b = &#39;Pancakes are lovely, dear!&#39;, c = TRUE, d = list(-99, 1)) There are two subsetting functions which you can apply to lists, [] and [[]]: To get a sublist: If you use [] with lists, you get a sublist. For example, if we enter ‘4’ for the last element in our list, we get a list in return which contains the list named ‘d’. str(fancy_list[4]) ## List of 1 ## $ d:List of 2 ## ..$ : num -99 ## ..$ : num 1 To get the element itself: If you use [[]], you get the element itself. For example, if we enter ‘4’ for the last element in our list, we get the list itself. str(fancy_list[[4]]) ## List of 2 ## $ : num -99 ## $ : num 1 If elements are named, you can get also get an element by using a shorthand involving the dollar sign. str(fancy_list$d) ## List of 2 ## $ : num -99 ## $ : num 1 You can do more complicated subsetting operations by combining [] and [[]]. Be very careful though. # returns the last element in a list str(fancy_list[4]) ## List of 1 ## $ d:List of 2 ## ..$ : num -99 ## ..$ : num 1 # returns the last element as itself str(fancy_list[[4]]) ## List of 2 ## $ : num -99 ## $ : num 1 # returns the first element of the last element as a list str(fancy_list[[4]][1]) ## List of 1 ## $ : num -99 # returns the first element of the last element as itself str(fancy_list[[4]][[1]]) ## num -99 1.3 Checking cases with conditionals Well, we know this stuff from Python already, but all of it looks slightly different in Python, so let’s dive right into it. 1.3.1 if- else An if-else structure is useful if you want to execute different code blocks depending on whether a certain statement is evaluated as TRUE or FALSE: recession = TRUE if (recession){ print(&quot;Booh!&quot;) } else { print(&quot;Yay!&quot;) } ## [1] &quot;Booh!&quot; In our case, the statement is very simple, it is just the value of our variable recession, which is TRUE. In this case, the code block is executed and R prints out ‘Booh!’ to the display. If the statement would have been evaluated as FALSE, the code within the else-bock would have been executed instead. Notice the difference to the Python code we discussed: Statements have to surrounded by round brackets. We have to use curly brackets to delimit our code blocks. This problem is solved by colon + indentation in Python. 1.3.2 if - else if - else If we want to check more than case, we can use the if - else if - else structure. color = &#39;violet&#39; if (color == &#39;red&#39;) { print(&#39;It is a tomatoe!&#39;) } else if (color == &#39;yellow&#39;) { print(&#39;It is a yellow pepper!&#39;) } else if (color == &#39;violet&#39;) { print(&#39;It is an onion!&#39;) } else { print(&#39;No idea what this is!&#39;) } ## [1] &quot;It is an onion!&quot; R checks each of the provided statements and executes the block of the first statement that is evaluated as true. 1.3.3 Checking multiple cases with switch If you want to pass options or have a lot of conditions to check, you can use the switch function. x = 1 y = 2 operation = &#39;plus&#39; switch(operation, plus = x + y, minus = x - y, times = x * y, divide = x / y, stop(&#39;You specified an unknown operation!&#39;) ) ## [1] 3 1.3.4 Comparison and logical operators We have already used the == operator which checks for equality between two values. You can also use the identical function. This makes sure that only one truth-value is returned. If more than one value is returned, say from a comparison of vectors with ==, R will check only the first truth-value. A = c(1, 2, 3, 4) B = c(1, 3, 4, 5) if (A == B) { print(&#39;They are equal!&#39;) } else { print(&#39;They are not equal!&#39;) } ## Warning in if (A == B) {: the condition has length &gt; 1 and only the first ## element will be used ## [1] &quot;They are equal!&quot; if (identical(A, B)) { print(&#39;They are equal!&#39;) } else { print(&#39;They are not equal!&#39;) } ## [1] &quot;They are not equal!&quot; A disadvantage of identical is that you have to be very specific regarding types: identical(0L, 0) ## [1] FALSE You also have the following other operators for logical comparisons: !=: not identical, &lt;: smaller than, &lt;=: smaller than or equal, &gt;: bigger than, &gt;=: bigger than or equal, !: not, &amp;&amp;: logical ‘and’, ||: logical ‘or’, is.logical etc. Finally, note that the use of doubles in logical comparisons can be dangerous. 1 - 1/3 - 1/3 - 1/3 == 0 ## [1] FALSE R exhibits this strange behavior because there are always approximation errors when using floating point numbers. 1 - 1/3 - 1/3 - 1/3 ## [1] 1.110223e-16 You can use the near function from the dplyr package to account for these cases. dplyr::near(1 - 1/3 - 1/3 - 1/3, 0) ## [1] TRUE 1.4 Functions You can of course also write functions in R. We will start by learning how to define them. 1.4.1 Function definitions To see how to define a function, let’s just write one. calc_percent_missing = function(x){ mean(is.na(x)) } calc_percent_missing(c(1, 2, 6, 3, 7, NA, 9, NA, NA, 1)) ## [1] 0.3 You can see that we need three things to define a function: a function name, in this case it’s calc_percent_missing, function arguments, in this case it’s just one, the vector x, the function body which is enclosed by curly parentheses. Note that the last statement that is evaluated in the function body is automatically taken as the return value of the function. 1.4.2 Function arguments and default values Function arguments in R can usually be broadly divided into two categories: data: either a dataframe or a vector, details: parameters which govern the computation. For the latter you often want to define default values. You can do this just as in Python and we will look at our old friend, the Cobb-Douglas utility function. cobb_douglas = function(x, a = 0.5, b = 0.5) { u = x[1]**a * x[2]**b } x = c(1, 2) print(cobb_douglas(x)) ## [1] 1.414214 print(cobb_douglas(x, b = 0.4, a = 0.6)) ## [1] 1.319508 Note that I not only overwrote the default values but also changed the order of the arguments. Similarly to Python, you can change the order of arguments if you call the arguments by their keywords. 1.4.3 Arbitrary number of arguments Sometimes you might want to write a function which takes an arbitrary number of arguments. You can do this with the dot-dot-dot argument. I demonstrate its usefulness with a nice little function by Hadley Wickham. Note that str_c is a function to combine strings into a single one. commas &lt;- function(...) { stringr::str_c(..., collapse = &quot;, &quot;) } commas(letters[1:10]) ## [1] &quot;a, b, c, d, e, f, g, h, i, j&quot; 1.4.4 Function returns I already told you that a function automatically returns the value of the last statement evaluated. You can however also be explicit about it by using the return function. 1.4.5 Pipes Piping is a practice that we will often use in this course to quickly chain a lot of functions together. I will tell you more about it later. For now, we only have to know under which conditions functions are pipeable. A function is pipeable if you pass an object to it and the function returns a modified version of the object. An example of a pipeable function would be one to which you pass a numeric vector and it returns it in a sorted form. A function is also pipeable if we pass an object to a function and the function returns it unmodified but creates what R programmers such as Hadley Wickham call a side-effect in the meantime. Such side-effects could be the drawing of a plot or the display of basic information about the object that is passed to the function. I will again illustrate this with a useful function by Wickham. We could program our function calc_percent_missing from above in such a way. calc_percent_missing = function(x) { n = mean(is.na(x)) cat(&quot;Percent missing: &quot;, n*100, &#39;!&#39;, sep=&#39;&#39;) invisible(x) } test_vector = c(1, 2, 3, NA, 6, 9, NA, NA, NA, 10) calc_percent_missing(test_vector) ## Percent missing: 40! As you can see, this function displays the percentage of missing values on the screen. It still returns the unchanged vector x, although it does not print it out. 1.5 Iterations Now, after we heard about functions and conditionals it is time to think about iterations again. We will first look at the kind of loops we also know from Python and then briefly look at the map function family. Before we actually start, let’s briefly create a dataframe or – more specifically – a tibble. # letters denote variable names my_df = tibble::tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) You will notice that you have now a new dataframe in your workspace, my_df. We will work with it in the following. 1.5.1 for loops 1.5.1.1 Looping over numeric indices Let’s say you want to calculate the median for each of the columns in our dataframe. You can of course do this by hand. median(my_df$a) ## [1] 0.07052297 median(my_df$b) ## [1] 0.3100978 median(my_df$c) ## [1] 0.4634452 median(my_df$d) ## [1] 0.3145689 This looks like a lot of repetition though, right? Maybe we can loop over the different columns. output = vector(&quot;double&quot;, ncol(my_df)) for (i in seq_along(my_df)) { output[i] = median(my_df[[i]]) } output ## [1] 0.07052297 0.31009782 0.46344516 0.31456887 The first line initializes an output vector. Make sure to always pre-allocate space for your output. In this case, we have pre-specified the length of the double vector to be equal to the number of columns in our dataframe. The second line contains the looping statement. The seq_along function acts as a safe alternative to length here. Why is it safer your ask? test_vector = vector(&quot;double&quot;, 0) seq_along(test_vector) ## integer(0) 1:length(test_vector) ## [1] 1 0 The body of the loop calculates the median for each of the dataframe columns. Note also the use of the subsetting operators here. Why did I use [[]] for subsetting the dataframe? 1.5.1.2 Looping over elements You can also write loops to iterate over elements. This is particularly useful in those cases where you do not want to store output. for (x in my_df) { print(median(x)) } ## [1] 0.07052297 ## [1] 0.3100978 ## [1] 0.4634452 ## [1] 0.3145689 In this case, I did not store the median values anywhere but just displayed them on the screen. Note that it would have been difficult to use this kind of loop for storing the elements in an initialized output vector as there is no natural way to index an output vector. 1.5.1.3 Looping over names Finally, note that you can also loop over the names of a vector. for (name in names(my_df)) { cat(&quot;Median for variable &quot;, name, &quot; is: &quot;, median(my_df[[name]]), &#39;\\n&#39;, sep=&#39;&#39;) } ## Median for variable a is: 0.07052297 ## Median for variable b is: 0.3100978 ## Median for variable c is: 0.4634452 ## Median for variable d is: 0.3145689 1.5.1.4 Repeat until with with Naturally, R also has a while structure which is particularly useful if you do not know how long a particular piece of code should be run. We had a longer discussion of this concept in the Python class, I will only provide one brief example here. The following loop prints out the square of a number that is provided by a user. If ‘q’ is entered, R leaves the loop. #while (TRUE) { # n &lt;- readline(prompt=&quot;Enter an integer (quit with q): &quot;) # if (n == &#39;q&#39;) { # break # } else { # cat(as.numeric(n), &#39; squared is &#39;, as.numeric(n)**2, &#39;!&#39;, sep = &#39;&#39;) # } #} 1.5.2 Split, apply, combine with map functions There is one cool feature which I briefly want to tell you about. Very often we want to loop over a vector, do something to each element of the vector and then save the results. You could also summarize this as split - apply - combine. The purrr package provides the functionalities for such operations in a handy manner. Let’s start with a basic example. 1.5.2.1 Basic example To see how it works let’s get back to a simple dataframe. To remind you (and me), I create a new one here. Note that the b column contains a missing value. my_df = tibble::tibble( a = rnorm(10), b = c(rnorm(9), NA), c = rnorm(10), d = rnorm(10) ) Let’s assume that we want to have the median of every column again. We already created a loop to do just that. There is a simpler way though. We can also use a map function to do this in a very compact manner. map_dbl(my_df, median) ## a b c d ## 0.69336239 NA 0.08664286 -0.01160225 We can compute the mean and standard deviation in a similar way. map_dbl(my_df, mean) ## a b c d ## 0.4504680 NA -0.1818221 0.2939560 map_dbl(my_df, sd) ## a b c d ## 1.0995427 NA 0.7371080 0.9824286 Pretty convenient, right? 1.5.2.2 Syntax While there are several map functions they all have the same argument structure: You have to pass the vector / list on whose elements you want to operate on. You have to pass the function which you want to apply to each element. You can also pass additional arguments to the map functions. Say we want to compute the mean for our b column as well. map_dbl(my_df, mean, na.rm = TRUE) ## a b c d ## 0.4504680 -0.4295373 -0.1818221 0.2939560 Finally, I already told you that there are several map functions, but so far we have only used the map_dbl function. These are the others: map(): makes a list, map_lgl(): makes a logical vector, map_int(): makes an integer vector, map_dbl(): makes a double vector, map_chr(): makes a character vector. The function choice depends on the output you expect. If we apply the mean to each element (i.e. column) of our dataframe, we usually expect to obtain a double vector that contains the means. But if we want a vector of strings, we can also do that. map_chr(my_df, mean, na.rm = TRUE) ## a b c d ## &quot;0.450468&quot; &quot;-0.429537&quot; &quot;-0.181822&quot; &quot;0.293956&quot; 1.5.2.3 map2 and pmap There are some extensions to the basic map functionality. For example, what do we have to do if we want to iterate over two lists? We can use the map2 function. Say we want to create a new dataframe but this time not all variables are suppose to have the same mean and standard deviation. Let’s create two vectors with our desired means and standard deviations. mu = list(0, 0, 0, 0) sd = list(1, 5, 10, 20) my_fancy_df = map2(mu, sd, rnorm, n = 20) my_fancy_df = set_names(my_fancy_df, c(&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;)) my_fancy_df = tibble::as_tibble(my_fancy_df) # look at the head of the dataframe head(my_fancy_df) ## # A tibble: 6 × 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.128 -0.996 -4.12 -14.1 ## 2 0.653 -8.21 0.552 30.4 ## 3 1.54 -0.00726 -0.649 -21.9 ## 4 0.668 -3.30 2.98 7.72 ## 5 0.422 -4.42 -2.06 12.7 ## 6 -2.01 -0.967 -2.04 36.5 OK. So what should we do if we want to iterate over more than two lists? Say we want to create vectors with different lengths, means and standard deviations. We could use pmap. mu = list(1, 2, 3, 4) sd = list(1, 10, 15, 20) n = list(1, 2, 3, 4) # enter arguments in the right order args = list(n, mu, sd) str(pmap(args, rnorm)) ## List of 4 ## $ : num 0.431 ## $ : num [1:2] 13.82 3.46 ## $ : num [1:3] -3.05 3.05 10.21 ## $ : num [1:4] -28.46 -4.99 -5.07 -11.3 And this is how our brief tour of R programming ends. Sources The exposition here is heavily inspired by the notes for a new book on R data science by Garrett Grolemund and Hadley Wickham. You can find detailed outlines here: http://r4ds.had.co.nz. "],["data-transformations-with-dplyr-and-tidyr.html", "Chapter 2 Data Transformations with dplyr and tidyr 2.1 Single table verbs 2.2 Using functions in combination with logical subsetting 2.3 The pipe operator 2.4 Multiple table verbs 2.5 Reshaping data with tidyr Sources", " Chapter 2 Data Transformations with dplyr and tidyr It is rare that we obtain a data set and it looks exactly as we want it. Usually, we drop variables, transform variables, merge data etc. While it is possible to perform these tasks using base R, there is now a package called dplyr which simplifies them enormously. Before we actually start working with it, we have to clean our workspace and load some packages. library(tibble) # nicer dataframes library(dplyr) # data transformations library(Ecdat) # Econ datasets While we are at it, let us also load a dataset that we will use in the following. It contains a cross-section of 601 individuals in the United States, some of their characteristics and how many extramarital affairs they had in the past year. affairs = as_data_frame(Fair) # from Ecdat ## Warning: `as_data_frame()` was deprecated in tibble 2.0.0. ## Please use `as_tibble()` instead. ## The signature and semantics have changed, see `?as_tibble`. head(affairs) ## # A tibble: 6 × 9 ## sex age ym child religious education occupation rate nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 male 37 10 no 3 18 7 4 0 ## 2 female 27 4 no 4 14 6 4 0 ## 3 female 32 15 yes 1 12 1 4 0 ## 4 male 57 15 yes 5 18 6 5 0 ## 5 male 22 0.75 no 2 17 6 3 0 ## 6 female 32 1.5 no 2 17 5 5 0 As you can see, the dataframe contains 9 variables. sex: factor: male or female age: age in years ym: number of years married child: factor: yes or no? religious: How religious from 1 (anti) to 5 (very)? education: education in years occupation: occupation, from 1 to 7, according to hollingshead classification rate: self rating of marriage, from 1 (very unhappy) to 5 (very happy) nbaffairs: number of affairs in past year 2.1 Single table verbs The dplyr package contains a number of single-table verbs which can be used to handle most of the day-to-day data work. The structure of the commands is always the same: verb(dataframe, operation). The output of the functions is again a dataframe. Let’s look at each of the verbs in turn. We start with filter. 2.1.1 filter to filter observations Filter can be used to select a subset of observations based on the truth value of a condition. Let’s say that we are only interested in the affairs of men. filter(affairs, sex == &quot;male&quot;) ## # A tibble: 286 × 9 ## sex age ym child religious education occupation rate nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 male 37 10 no 3 18 7 4 0 ## 2 male 57 15 yes 5 18 6 5 0 ## 3 male 22 0.75 no 2 17 6 3 0 ## 4 male 57 15 yes 2 14 4 4 0 ## 5 male 22 1.5 no 4 14 4 5 0 ## 6 male 37 15 yes 2 20 7 2 0 ## 7 male 27 4 yes 4 18 6 4 0 ## 8 male 47 15 yes 5 17 6 4 0 ## 9 male 37 4 yes 2 20 6 4 0 ## 10 male 42 15 yes 5 20 6 4 0 ## # … with 276 more rows As you can see, this gives us dataframe which just contains the observations for men. So, far we have just applied the function filter but have not saved the result anywhere. We need to assign it to a new dataframe or reassign it to the existing one. affairs_men = filter(affairs, sex == &quot;male&quot;) head(affairs_men) ## # A tibble: 6 × 9 ## sex age ym child religious education occupation rate nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 male 37 10 no 3 18 7 4 0 ## 2 male 57 15 yes 5 18 6 5 0 ## 3 male 22 0.75 no 2 17 6 3 0 ## 4 male 57 15 yes 2 14 4 4 0 ## 5 male 22 1.5 no 4 14 4 5 0 ## 6 male 37 15 yes 2 20 7 2 0 We can of course add further conditions. Let’s say we are only interested in men without children. affairs_childless_men = filter(affairs, sex == &quot;male&quot;, child == &quot;no&quot;) head(affairs_childless_men) ## # A tibble: 6 × 9 ## sex age ym child religious education occupation rate nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 male 37 10 no 3 18 7 4 0 ## 2 male 22 0.75 no 2 17 6 3 0 ## 3 male 22 1.5 no 4 14 4 5 0 ## 4 male 27 0.417 no 4 17 6 4 0 ## 5 male 22 4 no 3 16 5 5 0 ## 6 male 22 4 no 1 18 5 5 0 You can add as many conditions as you want, these are then linked by a logical ‘and’. You can also create more complicated statements with the following logical and relational operators: == equal != not equal ! not &amp; and | or &gt; greater &lt; smaller &gt;= greater or equal &lt;= smaller or equal So, let’s say we want to have the very religious, childless men. This can be accomplished as follows: affairs_childless_zealots = filter(affairs, sex == &quot;male&quot;, child == &quot;no&quot;, religious == 4 | religious == 5) So, there you go. One last thing. Sometimes you have missing values in your dataframe, which are denoted as NAs (not available). I will create a small dataframe for us in the following. na_df = tibble( x = c(1:10), y = c(1:2, NA, NA, 3:8) ) na_df ## # A tibble: 10 × 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 1 1 ## 2 2 2 ## 3 3 NA ## 4 4 NA ## 5 5 3 ## 6 6 4 ## 7 7 5 ## 8 8 6 ## 9 9 7 ## 10 10 8 To check for NA, we can use the function is.na which returns either TRUE or FALSE. We can exploit this, if we want to filter NAs out. filter(na_df, is.na(y)) # dataframe consisting of observations with NAs ## # A tibble: 2 × 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 3 NA ## 2 4 NA filter(na_df, !is.na(y)) # dataframe consisting of observations with non-missing values ## # A tibble: 8 × 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 1 1 ## 2 2 2 ## 3 5 3 ## 4 6 4 ## 5 7 5 ## 6 8 6 ## 7 9 7 ## 8 10 8 2.1.2 arrange to arrange a dataframe according to variables Next in line, we have the arrange command. It can be used to change the order of a dataframe according to one or several variables. For example, assume we want to sort the dataframe by sex and age. affairs = arrange(affairs, sex, age) To change the direction of ordering, use the desc function. affairs = arrange(affairs, sex, desc(age)) # sorted from old to young And that is everything there is to know about the arrange function. 2.1.3 select to select/drop variables Our next verb can be used to select variables from a dataframe. Before we start, let me quickly get rid of the dataframes that we do not need anymore. rm(affairs_childless_men, affairs_childless_zealots, affairs_men, na_df) So, let’s remind ourselves what is in our dataset. head(affairs) ## # A tibble: 6 × 9 ## sex age ym child religious education occupation rate nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female 57 15 yes 4 16 6 4 0 ## 2 female 57 15 yes 2 18 5 2 0 ## 3 female 57 15 yes 3 18 5 2 0 ## 4 female 57 15 no 4 20 6 5 0 ## 5 female 57 15 yes 1 18 5 4 2 ## 6 female 52 15 yes 5 12 1 3 0 We now create a new dataset consisting only of the variables sex, age, nbaffairs. head(select(affairs, sex, age, nbaffairs)) ## # A tibble: 6 × 3 ## sex age nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 57 0 ## 2 female 57 0 ## 3 female 57 0 ## 4 female 57 0 ## 5 female 57 2 ## 6 female 52 0 We could also have given the variables new names while selecting them. head(select(affairs, sex, age, number_of_affairs = nbaffairs)) ## # A tibble: 6 × 3 ## sex age number_of_affairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female 57 0 ## 2 female 57 0 ## 3 female 57 0 ## 4 female 57 0 ## 5 female 57 2 ## 6 female 52 0 Note however that there is a better function for renaming variables, rename, since it does not drop all the variables that are not explicitly mentioned. head(rename(affairs, number_of_affairs = nbaffairs)) ## # A tibble: 6 × 9 ## sex age ym child religious education occupation rate number_of_affai… ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female 57 15 yes 4 16 6 4 0 ## 2 female 57 15 yes 2 18 5 2 0 ## 3 female 57 15 yes 3 18 5 2 0 ## 4 female 57 15 no 4 20 6 5 0 ## 5 female 57 15 yes 1 18 5 4 2 ## 6 female 52 15 yes 5 12 1 3 0 If you want to keep most variables but just get rid of some, you can use the minus sign. The following command will keep all variables except rate: head(select(affairs, - rate)) ## # A tibble: 6 × 8 ## sex age ym child religious education occupation nbaffairs ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female 57 15 yes 4 16 6 0 ## 2 female 57 15 yes 2 18 5 0 ## 3 female 57 15 yes 3 18 5 0 ## 4 female 57 15 no 4 20 6 0 ## 5 female 57 15 yes 1 18 5 2 ## 6 female 52 15 yes 5 12 1 0 Vertical slicing is also possible. To get all variables from education to nbaffairs you can write. The minus operator works here as well. head(select(affairs, education:nbaffairs)) ## # A tibble: 6 × 4 ## education occupation rate nbaffairs ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 16 6 4 0 ## 2 18 5 2 0 ## 3 18 5 2 0 ## 4 20 6 5 0 ## 5 18 5 4 2 ## 6 12 1 3 0 head(select(affairs, -(education:nbaffairs))) ## # A tibble: 6 × 5 ## sex age ym child religious ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; ## 1 female 57 15 yes 4 ## 2 female 57 15 yes 2 ## 3 female 57 15 yes 3 ## 4 female 57 15 no 4 ## 5 female 57 15 yes 1 ## 6 female 52 15 yes 5 Note that select also works nicely to reorder the columns of a dataframe. For example, if we want to have the nbaffairs in first place, we can write. head(select(affairs, nbaffairs, everything())) ## # A tibble: 6 × 9 ## nbaffairs sex age ym child religious education occupation rate ## &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0 female 57 15 yes 4 16 6 4 ## 2 0 female 57 15 yes 2 18 5 2 ## 3 0 female 57 15 yes 3 18 5 2 ## 4 0 female 57 15 no 4 20 6 5 ## 5 2 female 57 15 yes 1 18 5 4 ## 6 0 female 52 15 yes 5 12 1 3 The function everthing is a function to capture all other variables except the ones explicitly mentioned. There are also some other helper functions to select all variables that start, end, or contain with a certain string. starts_with(“string”) ends_with(“string”) contains(“string”) matches(“reg_expression”) Here is one example. head(select(affairs, starts_with(&quot;r&quot;))) ## # A tibble: 6 × 2 ## religious rate ## &lt;int&gt; &lt;int&gt; ## 1 4 4 ## 2 2 2 ## 3 3 2 ## 4 4 5 ## 5 1 4 ## 6 5 3 That’s it. Now onwards to the mutate verb! 2.1.4 mutate to create new variables With mutate we can create new variables. For example, we might want to create a variable to capture the age at which a person got married. In our dataset we have two variables available to do just this: age (unfortunately, only measured in discrete steps, so our measure is somewhat imprecise) and ym. So, let’s do it. affairs = mutate(affairs, age_married = age - ym) You can use several functions to create new variables: arithmetic operators: +, -, *, /, ^ aggregate functions: you can use aggregate functions such as sum and mean which we will talk about later log: to create variables in logs, you can use log, log2, and log10 offsets: you can use lag and lead to refer to leading or lagged values, provided that your data are grouped (more about that later) logical comparisons: you can create Boolean variables using logical comparisons such as &lt;, &lt;=, &gt;=, !=, ==. 2.1.5 summarise to create summary tables The last single-table verb, we’ll get to know is summarise. It conjunction with aggregate functions such as mean it can be used to collapse our dataframe. summarise(affairs, age = mean(age, na.rm = TRUE)) ## # A tibble: 1 × 1 ## age ## &lt;dbl&gt; ## 1 32.5 As you can see, this collapses our dataframe to just one number, the average age of the persons in the sample. It gets more interesting if we structure our dataframe before applying summarise. Say we want to have the average age by sex. (By the way, why did we use the na.rm argument here?) affairs_grouped = group_by(affairs, sex) summarise(affairs_grouped, age = mean(age, na.rm = TRUE)) ## # A tibble: 2 × 2 ## sex age ## &lt;fct&gt; &lt;dbl&gt; ## 1 female 30.8 ## 2 male 34.3 Aha! Now, we have the mean ages for both men and women! Let’s go further by cross-tabulating sex and whether or not the person has children. affairs_grouped = group_by(affairs, sex, child) summarise(affairs_grouped, age = mean(age, na.rm = TRUE)) ## `summarise()` has grouped output by &#39;sex&#39;. You can override using the `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: sex [2] ## sex child age ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 female no 24.5 ## 2 female yes 33.7 ## 3 male no 28.7 ## 4 male yes 36.3 As you can see the results of the summarise verb depend on how we grouped our data beforehand. You can ungroup a dataframe by using the ungroup function: affairs_ungrouped = ungroup(affairs_grouped) summarise(affairs_ungrouped, age = mean(age, na.rm = TRUE)) # returns only one value again since data are ungrouped ## # A tibble: 1 × 1 ## age ## &lt;dbl&gt; ## 1 32.5 Of course, mean is not the only aggregate function that we can use in combination with summarise. Here are some more: sum(x): create group-wise sum for variable x median(x): create group-wise median for variable x sd(x): group-wise standard deviation for variable x min(x): minimum by group for variable x max(x): see above quantile(x, v): vth quantile for variable x by group first(x): first x-value in group by group last(x): last x-value in group by group n(): number of values in group by group n_distinct(x): number of distinct values in group by group 2.2 Using functions in combination with logical subsetting The Boolean values TRUE and FALSE are treated as 1 and 0, respectively. This makes them useful in combination with our aggregate functions. Say we want to have the total number of very religious people. We can use summarize for this. summarise(affairs, no_very_religious = sum(religious &gt;= 4)) ## # A tibble: 1 × 1 ## no_very_religious ## &lt;int&gt; ## 1 260 How does this work? Well the expression religious &gt;= 4 produces a vector of the same length as religious with TRUE and FALSE values. If we sum over these values, we add a 1 for every observation for which the statement is true. We can also use subsetting in a different way. Let’s say we want to have the mean age of all those very religious people. We can then write: summarise(affairs, mean_age = mean(age[religious &gt;= 4])) ## # A tibble: 1 × 1 ## mean_age ## &lt;dbl&gt; ## 1 34.5 OK, let’s try to understand this. The expression age[religious &gt;= 4] selects only those observations from the age vector for which religious &gt;= 4 is true. Then, the mean of these observations is calculated. You might get a feeling now how powerful the interplay of logical subsetting and the functions can be. 2.3 The pipe operator Now that we understand dplyr a little better, let’s make our code more beautiful and readible at the same time. I now introduce the pipe operator. Before, let’s quickly get rid of our mess. rm(list = c(&quot;affairs_grouped&quot;, &quot;affairs_ungrouped&quot;)) We now have our original dataframe again. Let’s say I want to select the variables age, ym, and nbaffairs, create the variable which gives us age at marriage and sort the data by the number of affairs and our new variables. The wordy way looks like this: affairs = select(affairs, age, ym, nbaffairs) affairs = mutate(affairs, age_married = age - ym) affairs = arrange(affairs, nbaffairs, age_married) head(affairs) ## # A tibble: 6 × 4 ## age ym nbaffairs age_married ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17.5 10 0 7.5 ## 2 22 7 0 15 ## 3 22 7 0 15 ## 4 22 7 0 15 ## 5 17.5 1.5 0 16 ## 6 17.5 0.75 0 16.8 Now, I don’t know about you but I have the feeling that I wrote the word affairs way too many times. This is because we a) have to reassign the new dataframe to the old dataframe so many times and b) always have to make sure that R knows on which dataframe we are using our verbs. Here, using the pipe operator comes in handy. Let’s first restore our original dataset. affairs = as_data_frame(Fair) # restore dataframe We can alternatively write the code as: affairs = affairs %&gt;% select(age, ym, nbaffairs) %&gt;% mutate(age_married = age - ym) %&gt;% arrange(nbaffairs, age_married) head(affairs) ## # A tibble: 6 × 4 ## age ym nbaffairs age_married ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 17.5 10 0 7.5 ## 2 22 7 0 15 ## 3 22 7 0 15 ## 4 22 7 0 15 ## 5 17.5 1.5 0 16 ## 6 17.5 0.75 0 16.8 Now, this look much nicer, right? We can skip the tedious reassignments and also don’t have to tell R on which dataframe we want to operate. You can read this as follows: Take the affairs dataframe and apply the select verb on it. The result is a new dataframe with the three variables. Take this dataframe of three variables and apply the mutate verb to it. The result is a new dataframe which also includes the variable age_married. Take the new dataframe and apply the arrange verb to it. The result is a new dataframe. Assign this new dataframe to our variable affairs. Neat, eh? Piping works so nicely, because the input and output of dplyr verbs is always a dataframe. You can use the pipe operator also with other packages and we will use it when it is reasonable to do so. 2.4 Multiple table verbs dplyr also features verbs which you can apply to two tables. These are usually functions to join or merge two datasets. 2.4.1 Mutating joins Mutating joins allow you to combine variables from multiple tables. To see how they work, let’s create two small tibbles. first_df = tibble( country = c(&#39;Afghanistan&#39;, &#39;Belgium&#39;, &#39;China&#39;, &#39;Denmark&#39;), population = c(33369945, 11371928, 1382323332, 5690750) ) # gdp in millions second_df = tibble( country = c(&#39;Afghanistan&#39;, &#39;Belgium&#39;, &#39;Denmark&#39;, &#39;Germany&#39;), gdp = c(35146, 422809, 211916, 3232545) ) head(first_df) ## # A tibble: 4 × 2 ## country population ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 ## 2 Belgium 11371928 ## 3 China 1382323332 ## 4 Denmark 5690750 head(second_df) ## # A tibble: 4 × 2 ## country gdp ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 35146 ## 2 Belgium 422809 ## 3 Denmark 211916 ## 4 Germany 3232545 2.4.1.1 left_join The left_join function matches rows from the second dataframe to the first dataframe. All rows from the first dataframe are kept. left_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 4 × 3 ## country population gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 35146 ## 2 Belgium 11371928 422809 ## 3 China 1382323332 NA ## 4 Denmark 5690750 211916 2.4.1.2 right_join The right_join function does exactly the opposite. It matches rows from the second dataframe to the first dataframe. All rows from the second dataframe are kept. right_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 4 × 3 ## country population gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 35146 ## 2 Belgium 11371928 422809 ## 3 Denmark 5690750 211916 ## 4 Germany NA 3232545 2.4.1.3 inner_join The inner_join function joins the data from both dataframes but only keeps those observations which exist in both dataframes. inner_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 3 × 3 ## country population gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 35146 ## 2 Belgium 11371928 422809 ## 3 Denmark 5690750 211916 2.4.1.4 full_join The full_join function also joins the data from both dataframes but keeps all observations. full_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 5 × 3 ## country population gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 35146 ## 2 Belgium 11371928 422809 ## 3 China 1382323332 NA ## 4 Denmark 5690750 211916 ## 5 Germany NA 3232545 2.4.2 Filtering joins Filtering joins are helpful if you want to filter the observations in one dataset based on whether they exist in the other dataset. 2.4.2.1 semi_join The semi_join function keeps all observations in the first dataframe which also exist in the second. semi_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 3 × 2 ## country population ## &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 ## 2 Belgium 11371928 ## 3 Denmark 5690750 2.4.2.2 anti_join The anti_join function drops all observations in the first dataframe which also exist in the second. anti_join(first_df, second_df, by = &quot;country&quot;) ## # A tibble: 1 × 2 ## country population ## &lt;chr&gt; &lt;dbl&gt; ## 1 China 1382323332 2.4.3 The by argument The by argument controls by what variables two dataframes are matched. If you do not specify it, dplyr uses all variables that exist in both tables, a so-called natural join. In our examples we could also have left the by argument unspecified since both dataframes only share the variable country. You can also pass character vector to specify by which variable the two dataframes are supposed to be matched. This is what we did in the preceding examples. Finally, what to do if the variable by which you want to match has different names in the two dataframes. Use a named character vector! first_df = tibble( country = c(&#39;Afghanistan&#39;, &#39;Belgium&#39;, &#39;China&#39;, &#39;Denmark&#39;), population = c(33369945, 11371928, 1382323332, 5690750) ) # gdp in millions second_df = tibble( country_name = c(&#39;Afghanistan&#39;, &#39;Belgium&#39;, &#39;Denmark&#39;, &#39;Germany&#39;), gdp = c(35146, 422809, 211916, 3232545) ) full_join(first_df, second_df, by = c(&#39;country&#39; = &#39;country_name&#39;)) ## # A tibble: 5 × 3 ## country population gdp ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 33369945 35146 ## 2 Belgium 11371928 422809 ## 3 China 1382323332 NA ## 4 Denmark 5690750 211916 ## 5 Germany NA 3232545 As you can see, this matched variable country in our first dataframe with country_name in our second dataframe. The variable name for the resulting dataframe is country. 2.5 Reshaping data with tidyr Another thing that happens very often is that the datasets we obtain are not in the right format. What do I mean by ‘right’ here? You already talked with Lachlan about this but some basic principles are: Every row is an observation. Every column is a variable. Each cell contains the value of a variable for a specific observation. Datasets which conform to these three principles are called tidy in the R world. There is now package called tidyr which helps you in converting untidy datasets to tidy ones. library(tidyr) In the following we will look at two cases which I will call dirty wide and dirty long datasets. 2.5.1 From dirty wide to tidy long Have a look at the following small dataset, containing population data. dirty_wide = as_data_frame(table4a) head(dirty_wide) ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 Are these tidy? No! First of all, we have column names which are actually variable values: 1999 and 2000. Secondly, it is not clear what data the cells contain. To reshape the data from dirty long to tidy long, we use the gather command. gather(dirty_wide, `1999`, `2000`, key = &#39;year&#39;, value = &#39;population&#39;) ## # A tibble: 6 × 3 ## country year population ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 ## 2 Brazil 1999 37737 ## 3 China 1999 212258 ## 4 Afghanistan 2000 2666 ## 5 Brazil 2000 80488 ## 6 China 2000 213766 How did this work? The first argument is the name of the dataset. The following arguments are the column names which are actually the values of a variable. We want these values to be stored in a variable called year so we write: key = 'year'. ‘Key’ in this context is just another word for variable name. Finally, we specify the variable name population for the values in our cells by writing value = 'population'. The result is a tidy long dataframe. 2.5.2 From dirty long to tidy wide We can also have a dirty long dataframe. Look at the following tibble. dirty_long = as_data_frame(table2) head(dirty_long) ## # A tibble: 6 × 4 ## country year type count ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 Again this dataframe is not tidy. The variable tidy contains variable names and the count variable the corresponding values. We want to reshape these data and create two variables cases and population with the cells containing the corresponding values. This can be accomplished using the spread command. spread(dirty_long, key = type, value = count) ## # A tibble: 6 × 4 ## country year cases population ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Afghanistan 1999 745 19987071 ## 2 Afghanistan 2000 2666 20595360 ## 3 Brazil 1999 37737 172006362 ## 4 Brazil 2000 80488 174504898 ## 5 China 1999 212258 1272915272 ## 6 China 2000 213766 1280428583 The first argument is the name of the dataframe again. Secondly, we specify the column, type in this case, which contains the variable names or keys. Finally, we specify the variable, count, which contains the values of the variables. The resulting dataframe is tidy again. Sources You can find the cheat sheet for dplyr and tidyr here: https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf. If you want to know more about the principles underlying tidy data, have a look at the following article by Hadley Wickham: https://www.jstatsoft.org/article/view/v059i10. There is also another packages called reshape2 which can do everything that tidyr can and more: https://cran.r-project.org/web/packages/reshape2/index.html. "],["visualizing-data-with-ggplot2.html", "Chapter 3 Visualizing data with `ggplot2 3.1 Introduction 3.2 Aesthetics 3.3 Facets: Subplots for different variable values 3.4 Geoms 3.5 Statistical tranformations with stats 3.6 Positions 3.7 Coordinate systems 3.8 Labeling 3.9 Scales 3.10 Themes 3.11 Saving graphs Sources", " Chapter 3 Visualizing data with `ggplot2 3.1 Introduction Before we actually start to do stuff, we should load some libraries. rm(list=ls()) # clean workspace library(&#39;ggplot2&#39;) # graphics library library(&#39;tibble&#39;) # nice dataframes 3.1.1 The dataset While we are at it, let us also load a dataset which is included in the ggplot library. diamonds_df = as_data_frame(diamonds) head(diamonds_df) # look at the head of the dataset ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 Let’s briefly look at this dataset. It includes the prices and features of around 54 000 round cut diamonds. It contains the following variables (Hint: you can also look up the description of the dataset using ?diamonds once you loaded ggplot2): price: price in US dollars carat: weight of the diamond cut: quality of the cut (Fair, Good, Very Good, Premium, Ideal) color: diamond colour from J (worst) to D (best) clarity: clarity of the diamond (l1, SI1, SI2, VS1, VS2, VVS1, VVS2, IF) x: length in mm y: width in mm z: depth in mm depth: total depth percentage table: width of top of diamond relative to widest point We will use this dataset in the following to learn the basics of ggplot2, a graphics package for R which can be used1 to create beautiful graphics. 3.1.2 Basic example: an ordinary scatterplot We will start to build up the logic of the ggplot2 command step by step. No matter which graphics you want to draw, you have to start as follows: ggplot(data = diamonds_df) As you can see, if you execute this command, this opens an empty graphics windows and nothing else happens. This make sense. So far we have only told ggplot which data we want to use for the plot. To actually see something we have to add a layer. Say we want to produce a scatter plot that shows us the association between the variables carat and price. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat , y = price)) What have we done here? We have added a new layer to our empty plot. The function geom_point specifies that we want to add a scatterplot layer. The mapping argument defines how data are mapped into the visual properties of the graph. This argument is always combined with the aes function which is a shorthand for aesthetics. In our case, we have specified to map the carat of the diamond to the x-axis and the price of the diamond to the y-axis. There seems to be a positive relationship between carat and price although it is noisier for higher carat levels. 3.2 Aesthetics 3.2.1 Aesthetic mappings Of course, we can do more by adding further variables to our plot. Say we want to color the dots according to the clarity category. For this, we have to add the color argument to the aesfunction. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity)) We have mapped the clarity variable to the color aesthetic and this gives us a clear pattern: For a given weight of the diamond, diamonds with higher clarity are more expensive. There are also other aesthetics and we could have mapped the variable into each of them. Some aesthetics are: size of the dots with size transparency of the dots with alpha shape of dots with shape Let’s try it out: ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, alpha = clarity)) Pretty cool, eh? You can also map more than simple variables to aesthetics. Here we produce a boolean which is TRUE or FALSE depending on whether clarity is above a certain threshold. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity &gt; &#39;VS2&#39;)) 3.2.2 Manually fixing aesthetics So, what if you just want to manually change an aesthetic without mapping a variable to it? For example, we might have a strong affinity to the color blue and prefer it to the depressing black of our graphs. To do that, we can just pass the name of the color as a string to the color argument and pass it outside of the aes function. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price), color = &quot;blue&quot;) You can do similar things with the other aesthetics: for size: size in mm for shape: shape of a point as a number for alpha: value between 0 and 1 3.3 Facets: Subplots for different variable values What if we want do not map all variables into aesthetics of one graph but want to have one graph for each value of a variable? For our dataset, we could for example be interested in how carat is associated with price for different values of the variable cut. For this, we need to split our plot into different facets (i.e. subplots) using the facet_wrap function. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price)) + facet_wrap(~ cut, nrow = 2) As you can see, for each value of the cut variable, we get a separate subplot. Note that you should pass a discrete variable to the facet_wrap function. The nrow argument specified into how many rows the plots are organized. We can also cross-tabulate subplots for different variables with the facet_grid function. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price)) + facet_grid(clarity ~ cut) You can also use facet_grid to basically replicate the plot generated with facet_wrap. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price)) + facet_grid(. ~ cut) 3.4 Geoms 3.4.1 What kind of plot do you have in mind? The use of geoms. So far we have just produced a scatterplot. We can of course also create other kinds of plots using different geometrical objects or geoms. For example, we could fit a smooth line to the data with geom_smooth. ggplot(data = diamonds_df) + geom_smooth(mapping = aes(x = carat, y = price)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Depending on the geometrical object, different aesthetics can be used. To see which aesthetics are available type ?geom_smooth. Moreover, you can also see which additional arguments are available and which default values are set. In the following, we will change the confidence intervals and map the variable cut to the color aesthetic. ggplot(data = diamonds_df) + geom_smooth(mapping = aes(x = carat, y = price, color = cut), level = 0.9) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 3.4.2 Multiple geoms in one plot We can also put multiple geoms in one graph by creating multiple layers. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price), size = 0.2, alpha = 0.5) + geom_smooth(mapping = aes(x = carat, y = price)) ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; As you can see, we have simply added another layer to our plot using the + operator. Still, this is a little redundant since we pass the same values for the same aesthetics two times. We can improve on this a little, by putting common aesthetics in the ggplot command. ggplot(data = diamonds_df, mapping = aes(x = carat, y = price)) + geom_point(size = 0.2, alpha = 0.5) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and formula &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; It’s the same graph but with less typing! Note that I could also have put additional aesthetics or overwritten the global ones in the geom_point and geom_smooth functions. There are tons of geoms from which you can choose your preferred one. Instead of explaining them all, I have distributed a pdf document with a very helpful cheat sheet. Feel free to play around with them! 3.5 Statistical tranformations with stats When you use ggplot, there is always a statistical tranformation - a stat - of the data running in the background. Let us look at the following two plots. The first should be familiar by now. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price)) What happens in the background? We have plotted the raw data for the variables carat and price on the x- and y-axis. Nevertheless, in the background ggplot used a statistal transformation. It took the data from our dataframe, applied the identity transformation, i.e. nothing is changed, and passed the transformed data to ggplot. Not so exiting, right? Let’s look at a second plot, a histogram for the variable clarity. ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = clarity)) The x axis plots the categories for clarity. The variable count is plotted on the y-axis. But you will not find the variable count in our dataset. Ggplot created a new dataframe with the variable clarity and the variable count for each category and plotted this to the figure. This is an example of the count stat! At first, stats might not seem like an important topic because ggplot usually uses sensible statistical transformations. However, sometimes you want to override the default stat. Consider the following dataset now. test_df &lt;- tibble( colors = c(&quot;green&quot;, &quot;blue&quot;, &quot;red&quot;, &quot;black&quot;, &quot;white&quot;), numbers = c(20, 40, 50, 66, 8) ) head(test_df) ## # A tibble: 5 × 2 ## colors numbers ## &lt;chr&gt; &lt;dbl&gt; ## 1 green 20 ## 2 blue 40 ## 3 red 50 ## 4 black 66 ## 5 white 8 What happens if we naively use the geom for histograms? ggplot(data = test_df) + geom_bar(mapping = aes(x = colors)) Well, this is not what we wanted. Ggplot did not realize that we already have summary statistics in this dataframe, therefore each color category it counted to appear once. We want the geom to use the data in the numbers variable for the y-axis. We need to use a different stat. ggplot(data = test_df) + geom_bar(mapping = aes(x = colors, y = numbers), stat = &quot;identity&quot;) Ah, this is much better. A second case where you might want to think about stats is when you want to override the default mapping from transformed values to aesthetics. Let’s take a look at the help page for geom_bar by typing ?geom_bar. If you scroll down you will see that geom_bar computes two variables by default, count and prop. Let’s say we want ggplot to use proportions, i.e. prop instead of counts for the y-axis. This can be accomplished as follows: ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = clarity, y = ..prop.., group = 1)) Here, we are telling ggplot to use the default statistical tranformation, but use the variable prop that is generated during the transformation for the y-axis. Again, there are tons of statistical transformations in ggplot. 3.6 Positions Ok, now it is time to talk about position adjustments. What happens in our histograms when we map a variable to the fill aesthetic? ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = cut, fill = clarity)) This is cool, right? The bars for each clarity category are stacked upon each other for each cut category. This is because ggplot automatically uses the stack position adjustement when drawing geom_bar. There are more options however. ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;identity&quot;, alpha = 0.2) You can see that the bars now all start at \\(y = 0\\) and are not stacked upon each other. Let’s look at another example: If you instead use the fill position, bars are stacked upon each other, but each stack of bars is forced to have the same height. ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;fill&quot;) Third, you can also use the dodge position to make a useful change to the bar plot. This places the bars next to each other. ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = cut, fill = clarity), position = &quot;dodge&quot;) Finally, there is also a useful position adjustment for scatter plots. When points are plotted in the scatter plot their coordinates are rounded to be placed on the grid. Then, it can happen that many points overlap each other. To remidy this, you can use the jitter position adjustement which adds a small amount of random noise to each point. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price), position = &quot;jitter&quot;) Of course there is more to learn, but for that please be referred to the help pages by typing for example ? position_jitter. 3.7 Coordinate systems The default coordinate system used by ggplot is the Cartesian one (coord_cartesian). I will only briefly introduce some other useful commands here. Rest assured that there is again much more to say here. You can quickly flip axes using the coord_flip command. Let’s just do this to one of our bar charts. ggplot(data = diamonds_df) + geom_bar(mapping = aes(x = cut)) + coord_flip() With the coord_trans function you plot data to a Cartesian coordinate system, where axes are transformed by a function. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price)) + coord_trans(x = &quot;log&quot;, y = &quot;log&quot;) Sometimes you want to have a fixed aspect ratio for your coordinate system, you can use the coord_fixed command to create a Cartesian coordinate system with a fixed aspect ratio between x and y units. 3.8 Labeling So far, we looked at the construction of graphs but did not change labels. We can do this using the labs function. In this case, we add a title and labels for the x-axis and y-axis as well as the legend. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity)) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) 3.9 Scales Apart from changing the labels you can also change the scales associated with your graph. Scales control how your data are mapped to the graph. These scales are usually applied automatically when you create a graph. Let’s take our graph from the previous section. Ggplot2 adds the following features in the background. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity)) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) + scale_x_continuous() + scale_y_continuous() + scale_color_discrete() As you can see, ggplot2 automatically adds sensible scales. The first word after the scale_ gives you the name of the aesthetic which is then followed by the name of the scale. Sometimes you can change graphs by passing arguments to the scale functions, sometimes you completely replace them by invoking different scale functions. We will talk here specifically about three things that we can tweak using scale functions: axis ticks, legend labels and color schemes. To change the axis ticks, you have to pass the breaks argument with the desired breaks to the scales. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity)) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) + scale_x_continuous(breaks = seq(0, 5, 0.5)) + scale_y_continuous() + scale_color_discrete() You can also change the labels by passing the labels argument to the scale with the vector of desired labels. Next, we want to change the labels for our legend. These are related to the color aesthetic. We can just pass the labels argument with the vector of desired labels. In this example, I just pass a vector of numbers. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity), size = 0.2) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) + scale_x_continuous() + scale_y_continuous() + scale_color_discrete(labels = seq(1, 8)) Finally, what if you want to change the color of our graph? It is obviously related to the color aesthetic. In this case, we don’t have to pass an argument to the scale function but instead have to completely replace the scale. # brewer scales at http://colorbrewer2.org ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity), size = 0.2) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) + scale_x_continuous() + scale_y_continuous() + scale_color_brewer(palette = &quot;Reds&quot;) If you want to manually set colors, you can do so with the scale_color_manual function. 3.10 Themes You can also change the complete non-data elements of your graph by applying the theme function. You have the following themes available: theme_bw: white background with grid lines theme_classic: classic theme; axes but not grid lines theme_dark: dark background for contrast theme_gray: default theme with grey background theme_light: light axes and grid lines theme_linedraw: only black lines theme_minimal: minimal theme, no background theme_void: empty theme, only geoms are visible There is also a package called ggthemes which gives you a ton of other templates. In the following, we just switch our graph to the classic theme. ggplot(data = diamonds_df) + geom_point(mapping = aes(x = carat, y = price, color = clarity), size = 0.2) + labs(title = &quot;Relationship between weight and price of diamonds&quot;, x = &quot;Weight in carat&quot;, y = &quot;Price in dollars&quot;, color = &quot;Clarity&quot;) + scale_color_brewer(palette = &quot;Reds&quot;) + theme_classic() 3.11 Saving graphs We have not yet talked about how to save a plot to disk. This can be done using the ggsave function. Look up the help file for ggsave to see into which formats you can export your plot. Sources The exposition here is heavily inspired by the notes for a new book on R data science by Garrett Grolemund and Hadley Wickham. You can find detailed outlines here: http://r4ds.had.co.nz. The cheat sheet for ggplot2 is from the official RStudio website: https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf. Finally, if you want to know more about the details of ggplot2 there is a complete book on it by Hadley Wickham: https://www.amazon.de/ggplot2-Elegant-Graphics-Data-Analysis/dp/331924275X/ref=sr_1_1?ie=UTF8&amp;qid=1474400904&amp;sr=8-1&amp;keywords=ggplot2. "],["factor-variables.html", "Chapter 4 Factor Variables 4.1 Creating Factors 4.2 Some other useful functions from forcats", " Chapter 4 Factor Variables 4.1 Creating Factors library(tibble) library(dplyr) library(ggplot2) my_dataframe = tibble( age = c(12, 17, 11, 8), gender = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;), review = c(&quot;C&quot;, &quot;B&quot;, &quot;B&quot;, &quot;A&quot;) ) my_dataframe = my_dataframe %&gt;% mutate(gender = factor(gender, levels = c(&quot;Male&quot;, &quot;Female&quot;))) %&gt;% mutate(review = factor(review, levels = c(&quot;C&quot;, &quot;B&quot;, &quot;A&quot;), ordered = TRUE)) head(my_dataframe) ## # A tibble: 4 × 3 ## age gender review ## &lt;dbl&gt; &lt;fct&gt; &lt;ord&gt; ## 1 12 Male C ## 2 17 Male B ## 3 11 Female B ## 4 8 Female A So, we created an ordered as well as an unordered factor. If you want to see the values for the factor levels, use levels. levels(my_dataframe$gender) ## [1] &quot;Male&quot; &quot;Female&quot; levels(my_dataframe$review) ## [1] &quot;C&quot; &quot;B&quot; &quot;A&quot; 4.2 Some other useful functions from forcats library(forcats) 4.2.1 Modify factor levels my_dataframe = my_dataframe %&gt;% mutate(review = fct_recode(review, &quot;Very Good&quot; = &quot;A&quot;, &quot;Good&quot; = &quot;B&quot;, &quot;Bad&quot; = &quot;C&quot;)) head(my_dataframe) ## # A tibble: 4 × 3 ## age gender review ## &lt;dbl&gt; &lt;fct&gt; &lt;ord&gt; ## 1 12 Male Bad ## 2 17 Male Good ## 3 11 Female Good ## 4 8 Female Very Good No bad people. my_dataframe = my_dataframe %&gt;% mutate(review = fct_recode(review, &quot;Good&quot; = &quot;Bad&quot;)) head(my_dataframe) ## # A tibble: 4 × 3 ## age gender review ## &lt;dbl&gt; &lt;fct&gt; &lt;ord&gt; ## 1 12 Male Good ## 2 17 Male Good ## 3 11 Female Good ## 4 8 Female Very Good my_dataframe = my_dataframe %&gt;% group_by(gender) %&gt;% summarize(mean_age = mean(age, na.rm = TRUE)) %&gt;% mutate(gender = fct_reorder(gender, mean_age)) ggplot(my_dataframe) + geom_point(aes(x = mean_age, y = gender)) "],["reading-and-writing-data.html", "Chapter 5 Reading and Writing Data 5.1 Reading csv files with readr 5.2 Parsing data correctly with the parse functions 5.3 Parsing and reading at the same time 5.4 Writing csv files 5.5 Write and read RDS files with readr 5.6 Reading in Excel sheets and Stata data with readxl and haven Sources", " Chapter 5 Reading and Writing Data So far we have only used datasets from R packages or created toy tibbles. In this section, we will learn how to read in data from a variety of sources. 5.1 Reading csv files with readr We will start with the readr packages which is useful to a) read in csv files and to b) correctly parsing data columns. Before we start, let’s load some packages. library(readr) library(tibble) library(dplyr) 5.1.1 Reading in *-delimited data The readr package provides several functions to read in delimited data: read_csv(): comma delimited read_csv2(): semicolon delimited read_tsv(): tab delimited read_delim(): any delimiter To see how they work, let’s create some data and read them in. (I know, I know still no real data. Be patient!) my_csv = &quot;a, b, c, d 1, 2, 3, 4 5, 6, 7, 8&quot; read_csv(my_csv) ## Rows: 2 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (4): a, b, c, d ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 2 × 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 3 4 ## 2 5 6 7 8 As you can see, the function correctly interpreted the first line of our string as variable names and the remaining elements as comma-separated integers. The other functions work in a similar way. There is one exception though: read_delim. It allows for more general specifications. my_csv = &quot;a_ b_ c_ d 1_ 2_ Maria_ female 5_ 6_ Teresa_ female&quot; read_delim(my_csv, delim = &quot;_&quot;, trim_ws = TRUE) ## Rows: 2 Columns: 4 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;_&quot; ## chr (2): c, d ## dbl (2): a, b ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 2 × 4 ## a b c d ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 2 Maria female ## 2 5 6 Teresa female In this case, we specified an underscore as the delimiter and told the function to trim leading and trailing whitespace. Sometimes the csv file includes lines which we want readr to ignore. To do this we use the skip argument. my_csv = &quot;Sometimes you can read some rubbish here We don&#39;t want to import this name, age Julian, 29&quot; read_csv(my_csv, skip = 2) ## Rows: 1 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): name ## dbl (1): age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 1 × 2 ## name age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Julian 29 At other times, the csv file does not provide variable names, we can provide them by passing a character vector as the col_names argument. my_csv = &quot;Julian, 29\\nTeresa, 25&quot; read_csv(my_csv, col_names = c(&quot;Name&quot;, &quot;Age&quot;)) ## Rows: 2 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Name ## dbl (1): Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 2 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Julian 29 ## 2 Teresa 25 There are more options and you can explore them by looking them up in the help file. For now, I will only show you one more useful option: You can use the na argument to specify the characters used in the csv file to indicate missing values. my_csv = &quot;Julian, 29 Teresa, 25 Jonas, . ., 64&quot; read_csv(my_csv, col_names = c(&quot;Name&quot;, &quot;Age&quot;), na = &quot;.&quot;) ## Rows: 4 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Name ## dbl (1): Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 4 × 2 ## Name Age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Julian 29 ## 2 Teresa 25 ## 3 Jonas NA ## 4 &lt;NA&gt; 64 5.2 Parsing data correctly with the parse functions Sometimes columns are not correctly interpreted when they are read in. For these vectors, we can parse them differently using the parse functions. Each of these functions takes a vector and returns a vector. For example, assume in the following that I want to parse the age column not as an integer vector but as a character vector. I can use the parse_character function for this. my_csv = &quot;Julian, 29 Teresa, 25 Jonas, . ., 64&quot; my_tibble = read_csv(my_csv, col_names = c(&quot;name&quot;, &quot;age&quot;), na = &quot;.&quot;) ## Rows: 4 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): name ## dbl (1): age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(my_tibble) ## # A tibble: 4 × 2 ## name age ## &lt;chr&gt; &lt;dbl&gt; ## 1 Julian 29 ## 2 Teresa 25 ## 3 Jonas NA ## 4 &lt;NA&gt; 64 str(parse_character(my_tibble$age)) ## Error in parse_vector(x, col_character(), na = na, locale = locale, trim_ws = trim_ws): is.character(x) is not TRUE There are a bunch of functions, each for a different kind of data type: parse_logical parse_number (parse_double, parse_integer) parse_character parse_datetime (parse_date, parse_datetime) Let’s look at some of them in the following. 5.2.1 Parsing numbers The parse_number function is pretty amazing. It can recover number vectors from almost anything! Just look at the following example. my_column = c(&quot;100$&quot;, &quot;20%&quot;, &quot;Something with a 100&quot;) parse_number(my_column) ## [1] 100 20 100 You use the locale function to control for country-specific delimiters for the decimal point and grouping. # comma instead of decimal point my_column = c(&quot;1,23&quot;, &quot;1,23&quot;, &quot;1,245&quot;) parse_number(my_column, locale = locale(decimal_mark = &quot;,&quot;)) ## [1] 1.230 1.230 1.245 # &#39; to group numbers my_column = c(&quot;123&#39;456&#39;789&quot;) parse_number(my_column, locale = locale(grouping_mark = &quot;&#39;&quot;)) ## [1] 123456789 5.2.2 Parsing characters You would think parsing characters is pretty easy. There can be difficulties though because not everybody uses the same character encoding. To learn more about this topic, take a look at this website: http://www.w3.org/International/articles/definitions-characters/. We only need to know that different encodings exist and they can lead to problems. R usually assumes to ‘UTF-8’ encoding (and you should use it too!). See what happens if we read in characters with Latin-1 encoding: x1 = &quot;El Ni\\xf1o was particularly bad this year&quot; parse_character(x1) ## [1] &quot;El Ni\\xf1o was particularly bad this year&quot; Well, that does not look nice. But luckily enough, we can use the locale function to tell readr that the string is encoded with Latin-1. parse_character(x1, locale = locale(encoding = &quot;Latin1&quot;)) ## [1] &quot;El Niño was particularly bad this year&quot; Now, this time the parsing is correct! You can also use readr to try to guess the encoding with the guess_encoding function. Look up its help file if you want to know more. 5.2.3 Parsing dates # specify time zone maybe parse_datetime(&quot;2016-09-08T0708&quot;) ## [1] &quot;2016-09-08 07:08:00 UTC&quot; parse_datetime(&quot;20160908T0708&quot;) ## [1] &quot;2016-09-08 07:08:00 UTC&quot; parse_date(&quot;2016-09-08&quot;) ## [1] &quot;2016-09-08&quot; parse_date(&quot;2016/09/08&quot;) ## [1] &quot;2016-09-08&quot; library(hms) parse_time(&quot;01:10 am&quot;) ## 01:10:00 parse_date(&quot;27/05/1987&quot;, &quot;%d/%m/%Y&quot;) ## [1] &quot;1987-05-27&quot; 5.3 Parsing and reading at the same time Each parse function has a corresponding col function. This allows you to use the parse function to find out how to correctly parse a column and then specify the correct parsing right at the beginning of the data processing using the corresponding col function. I usually read in data in three steps. First, I read in all columns as character vectors. This allows me to browse the data and determine the correct parsing. To read in every column as a character vector, you can use the .default argument in the col_types function. challenge1 = read_csv(readr_example(&quot;challenge.csv&quot;), # there&#39;s an example dataset in the readr package called challenge.csv col_types = cols( .default = col_character() )) I can try out different parsers using the parse functions. (Note that you can use the parse functions from the readr package together with other packages such as readxl). In this case, browsing and parsing will lead you to conclude that the correct parsers are parse_double and parse_data, respectively. Finally, we specify the correct parsers directly at the beginning of the data processing stage using the col functions that correspond to the parse functions. challenge2 = read_csv(readr_example(&quot;challenge.csv&quot;), col_types = cols( x = col_double(), y = col_date() )) head(challenge2) ## # A tibble: 6 × 2 ## x y ## &lt;dbl&gt; &lt;date&gt; ## 1 404 NA ## 2 4172 NA ## 3 3004 NA ## 4 787 NA ## 5 37 NA ## 6 2332 NA 5.4 Writing csv files You can use the readr package to write csv files as well. In this case, we want to save our cleaned up dataframe as a csv file in a dataframes sub-folder. For this, we first check whether the sub-folder already exists. If it does not, we create it. if (!file.exists(&quot;dataframes&quot;)) { dir.create(&quot;dataframes&quot;) } Then, we write the csv file using the write_csv function. write_csv(challenge2, &quot;./dataframes/challenge2.csv&quot;) You can check now in your working directory whether this worked. Note that a csv file does not store the information about the correct parsing of the data columns. read_csv(&quot;./dataframes/challenge2.csv&quot;) ## Rows: 2000 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (1): x ## date (1): y ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## # A tibble: 2,000 × 2 ## x y ## &lt;dbl&gt; &lt;date&gt; ## 1 404 NA ## 2 4172 NA ## 3 3004 NA ## 4 787 NA ## 5 37 NA ## 6 2332 NA ## 7 2489 NA ## 8 1449 NA ## 9 3665 NA ## 10 3863 NA ## # … with 1,990 more rows We have to specify the correct parsing again! If you only work with R and the dataframe is not too big, you can store the dataframe as an RDS file instead. 5.5 Write and read RDS files with readr There is not much to say here apart from the fact that the RDS file ‘remembers’ the correct parsing. write_rds(challenge2, &quot;./dataframes/challenge2.rds&quot;) challenge_rds = read_rds(&quot;./dataframes/challenge2.rds&quot;) head(challenge_rds) ## # A tibble: 6 × 2 ## x y ## &lt;dbl&gt; &lt;date&gt; ## 1 404 NA ## 2 4172 NA ## 3 3004 NA ## 4 787 NA ## 5 37 NA ## 6 2332 NA 5.6 Reading in Excel sheets and Stata data with readxl and haven Many datasets are stored in Excel sheets. You can read them in using the readxl package. A similar package, haven, exists for Stata files. I will not use them here, but can look at the helpfiles for the commands read_excel and read_dta. library(readxl) help(read_excel) library(haven) help(read_dta) If you want to try them out, you can use the data files from the ‘Baby-Wooldridge’ here: http://www.cengage.com/aise/economics/wooldridge_3e_datasets/. Sources The exposition here is inspired by the notes for a new book on R data science by Garrett Grolemund and Hadley Wickham. You can find detailed outlines here: http://r4ds.had.co.nz. "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 String combine with str_c 6.2 Substrings with str_sub 6.3 Capitalize or not 6.4 String sort 6.5 Handling NAs", " Chapter 6 Working with Strings First load the package. library(stringr) 6.1 String combine with str_c our_string = c(&quot;With&quot;, &quot;great&quot;, &quot;power&quot;, &quot;comes&quot;, &quot;great&quot;, &quot;responsibility&quot;) str_length(our_string) ## [1] 4 5 5 5 5 14 str_c(&quot;Hello&quot;, &quot;World&quot;, sep = &quot; &quot;) ## [1] &quot;Hello World&quot; str_c(our_string) ## [1] &quot;With&quot; &quot;great&quot; &quot;power&quot; &quot;comes&quot; ## [5] &quot;great&quot; &quot;responsibility&quot; str_c(our_string, collapse = &quot; &quot;) ## [1] &quot;With great power comes great responsibility&quot; 6.2 Substrings with str_sub str_sub(our_string, 1, 1) ## [1] &quot;W&quot; &quot;g&quot; &quot;p&quot; &quot;c&quot; &quot;g&quot; &quot;r&quot; str_sub(our_string, 1, 2) ## [1] &quot;Wi&quot; &quot;gr&quot; &quot;po&quot; &quot;co&quot; &quot;gr&quot; &quot;re&quot; str_sub(our_string, -1, -1) ## [1] &quot;h&quot; &quot;t&quot; &quot;r&quot; &quot;s&quot; &quot;t&quot; &quot;y&quot; str_sub(our_string, -2, -1) ## [1] &quot;th&quot; &quot;at&quot; &quot;er&quot; &quot;es&quot; &quot;at&quot; &quot;ty&quot; 6.3 Capitalize or not str_to_lower(our_string) ## [1] &quot;with&quot; &quot;great&quot; &quot;power&quot; &quot;comes&quot; ## [5] &quot;great&quot; &quot;responsibility&quot; str_to_upper(our_string) ## [1] &quot;WITH&quot; &quot;GREAT&quot; &quot;POWER&quot; &quot;COMES&quot; ## [5] &quot;GREAT&quot; &quot;RESPONSIBILITY&quot; str_to_title(our_string) ## [1] &quot;With&quot; &quot;Great&quot; &quot;Power&quot; &quot;Comes&quot; ## [5] &quot;Great&quot; &quot;Responsibility&quot; # specify locale to be sure that behavior is consistent with locale option str_to_title(our_string, locale = &quot;en&quot;) ## [1] &quot;With&quot; &quot;Great&quot; &quot;Power&quot; &quot;Comes&quot; ## [5] &quot;Great&quot; &quot;Responsibility&quot; # capitalize first letter str_sub(our_string, 1, 1) &lt;- str_to_upper(str_sub(our_string, 1, 1)) our_string ## [1] &quot;With&quot; &quot;Great&quot; &quot;Power&quot; &quot;Comes&quot; ## [5] &quot;Great&quot; &quot;Responsibility&quot; 6.4 String sort str_sort(our_string) ## [1] &quot;Comes&quot; &quot;Great&quot; &quot;Great&quot; &quot;Power&quot; ## [5] &quot;Responsibility&quot; &quot;With&quot; 6.5 Handling NAs another_string = c(NA, &quot;Hello&quot;, &quot;World&quot;) str_c(another_string, collapse = &quot; &quot;) ## [1] NA "],["linear-regression-essentials-with-r.html", "Chapter 7 Linear Regression Essentials with R 7.1 Libraries 7.2 Loading the data 7.3 Meaningful names 7.4 Create variables for estimation 7.5 Summary statistics 7.6 Create three samples 7.7 Run the estimation for Table 1 in MRW (1992) 7.8 Show the results in a table 7.9 Robust standard errors 7.10 Some Graphs", " Chapter 7 Linear Regression Essentials with R 7.1 Libraries library(haven) # reading stata data library(dplyr) # data manipulation library(tibble) # nicer dataframes library(stargazer) # tables library(ggplot2) # graphs 7.2 Loading the data mrw_df = read_dta(&#39;data/mrw.dta&#39;) head(mrw_df) ## # A tibble: 6 × 11 ## number country n i o rgdpw60 rgdpw85 gdpgrowth popgrowth i_y ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Algeria 1 1 0 2485 4371 4.80 2.60 24.1 ## 2 2 Angola 1 0 0 1588 1171 0.800 2.10 5.80 ## 3 3 Benin 1 0 0 1116 1071 2.20 2.40 10.8 ## 4 4 Botswana 1 1 0 959 3671 8.60 3.20 28.3 ## 5 5 Burkina Fa… 1 0 0 529 857 2.90 0.900 12.7 ## 6 6 Burundi 1 0 0 755 663 1.20 1.70 5.10 ## # … with 1 more variable: school &lt;dbl&gt; We can cutify the output a little by using the kable function. knitr::kable(head(mrw_df)) number country n i o rgdpw60 rgdpw85 gdpgrowth popgrowth i_y school 1 Algeria 1 1 0 2485 4371 4.8 2.6 24.1 4.5 2 Angola 1 0 0 1588 1171 0.8 2.1 5.8 1.8 3 Benin 1 0 0 1116 1071 2.2 2.4 10.8 1.8 4 Botswana 1 1 0 959 3671 8.6 3.2 28.3 2.9 5 Burkina Faso 1 0 0 529 857 2.9 0.9 12.7 0.4 6 Burundi 1 0 0 755 663 1.2 1.7 5.1 0.4 As you can see, we have 11 variables and 121 observations. We have the following variables: number: a country identifier between 1 and 121 country country name (a string variable) country: the name of the country n: a dummy variable equal to one if the country is included in the non-oil sample i: a dummy variable equal to one if the country is included in the intermediate sample o: a dummy variable equal to one if the country is included in the oecd sample rgdpw60: real GDP per working age population in 1960 rgdpw85: real GDP per working age population in 1985 gdpgrowth: average annual growth rate of real GDP per working age population between 1960 and 1985 popgrowth: average annual growth rate of the working age population between 1960 and 1985 i_y: real investment as a share of real GDP, averaged over the period 1960-85 school: % of working age population in secondary school 7.3 Meaningful names The first thing we should do is probably to give these variables more meaningful names in order to escape the 90s charme conveyed by them. mrw_df = mrw_df %&gt;% rename(non_oil = n, oecd = o, intermediate = i, gdp_60 = rgdpw60, gdp_85 = rgdpw85, gdp_growth_60_85 = gdpgrowth, pop_growth_60_85 = popgrowth, inv_gdp = i_y, school = school) 7.4 Create variables for estimation In order to follow the estimation, we will need to create some additional variables: The logs of the GDP per working age pop. in 1985 and 1960. The investment to GDP ratio has to be converted to lie between 0 and 1. Also we need the log of it. We have to create the ndg variable which is assumed to be population growth (0 - 1) + 0.05. Again, we need the log of it. We want to use the log of the schooling rate (again first divided by 100). Finally, and just for consistency, we should convert our sample dummies to factors. # log gdp mrw_df = mrw_df %&gt;% mutate(ln_gdp_85 = log(gdp_85), ln_gdp_60 = log(gdp_60), ln_inv_gdp = log(inv_gdp/100), non_oil = factor(non_oil), intermediate = factor(intermediate), oecd = factor(oecd), ln_ndg = log(pop_growth_60_85/100 + 0.05), ln_school = log(school/100)) %&gt;% select(country, ln_gdp_85, ln_gdp_60, ln_inv_gdp, non_oil, intermediate, oecd, ln_ndg, ln_school, gdp_growth_60_85) head(mrw_df) ## # A tibble: 6 × 10 ## country ln_gdp_85 ln_gdp_60 ln_inv_gdp non_oil intermediate oecd ln_ndg ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 Algeria 8.38 7.82 -1.42 1 1 0 -2.58 ## 2 Angola 7.07 7.37 -2.85 1 0 0 -2.65 ## 3 Benin 6.98 7.02 -2.23 1 0 0 -2.60 ## 4 Botswana 8.21 6.87 -1.26 1 1 0 -2.50 ## 5 Burkina Faso 6.75 6.27 -2.06 1 0 0 -2.83 ## 6 Burundi 6.50 6.63 -2.98 1 0 0 -2.70 ## # … with 2 more variables: ln_school &lt;dbl&gt;, gdp_growth_60_85 &lt;dbl&gt; 7.5 Summary statistics Maybe, we would like to have summary statistics for our dataframe. For that, we need summary. summary(mrw_df) ## country ln_gdp_85 ln_gdp_60 ln_inv_gdp non_oil ## Length:121 Min. : 6.021 Min. : 5.948 Min. :-3.194 0:23 ## Class :character 1st Qu.: 7.098 1st Qu.: 6.881 1st Qu.:-2.120 1:98 ## Mode :character Median : 8.155 Median : 7.582 Median :-1.732 ## Mean : 8.106 Mean : 7.654 Mean :-1.815 ## 3rd Qu.: 8.949 3rd Qu.: 8.360 3rd Qu.:-1.423 ## Max. :10.152 Max. :11.263 Max. :-0.997 ## NA&#39;s :13 NA&#39;s :5 ## intermediate oecd ln_ndg ln_school gdp_growth_60_85 ## 0:46 0:99 Min. :-2.937 Min. :-5.521 Min. :-0.900 ## 1:75 1:22 1st Qu.:-2.703 1st Qu.:-3.730 1st Qu.: 2.800 ## Median :-2.604 Median :-3.006 Median : 3.900 ## Mean :-2.629 Mean :-3.204 Mean : 4.094 ## 3rd Qu.:-2.538 3rd Qu.:-2.504 3rd Qu.: 5.300 ## Max. :-2.137 Max. :-2.112 Max. : 9.200 ## NA&#39;s :14 NA&#39;s :3 NA&#39;s :4 7.6 Create three samples mrw_oecd = mrw_df %&gt;% filter(oecd == 1) mrw_int = mrw_df %&gt;% filter(intermediate == 1) mrw_non_oil = mrw_df %&gt;% filter(non_oil == 1) 7.7 Run the estimation for Table 1 in MRW (1992) To run a linear model we need the lm command. m_non_oil = lm(ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_non_oil) m_int = lm(ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_int) m_oecd = lm(ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_oecd) To get nicely formatted results, we can use the summary command: summary(m_non_oil) ## ## Call: ## lm(formula = ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_non_oil) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.79144 -0.39367 0.04124 0.43368 1.58046 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.4299 1.5839 3.428 0.000900 *** ## ln_inv_gdp 1.4240 0.1431 9.951 &lt; 2e-16 *** ## ln_ndg -1.9898 0.5634 -3.532 0.000639 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6891 on 95 degrees of freedom ## Multiple R-squared: 0.6009, Adjusted R-squared: 0.5925 ## F-statistic: 71.51 on 2 and 95 DF, p-value: &lt; 2.2e-16 7.8 Show the results in a table stargazer(m_non_oil, m_int, m_oecd, type = &quot;latex&quot;) % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Mon, Sep 27, 2021 - 17:19:30 stargazer(m_non_oil, m_int, m_oecd, type = &quot;latex&quot;, column.labels = c(&quot;Non-Oil&quot;, &quot;Intermediate&quot;, &quot;OECD&quot;), covariate.labels = c(&quot;$\\\\log(\\\\frac{I}{GDP})$&quot;, &quot;$\\\\log(n+\\\\delta+g)$&quot;, &quot;Constant&quot;), dep.var.labels = &quot;Log(GDP) 1985&quot;, omit.stat = c(&quot;f&quot;, &quot;rsq&quot;, &quot;ser&quot;), title = &quot;Replication of (part of) Table 1 in Mankiw, Romer, and Weil (1992)&quot;, style = &quot;qje&quot;) % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Mon, Sep 27, 2021 - 17:19:30 7.9 Robust standard errors In economics, we often would like to have robust standard errors. To look at how we see them. Let’s go back to an example. lm_example = lm(ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_non_oil) library(sandwich) # for robust standard errors library(lmtest) # to nicely summarize the results ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric lm_robust = coeftest(lm_example, vcov = vcovHC(lm_example, &quot;HC1&quot;)) print(lm_robust) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.42988 1.58558 3.4246 0.0009108 *** ## ln_inv_gdp 1.42401 0.13196 10.7911 &lt; 2.2e-16 *** ## ln_ndg -1.98977 0.54524 -3.6493 0.0004297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Since we do not want to type this every time. We should write a short function that takes a linear model and returns the robust summary of it. # needs sandwich and lmtest print_robust = function(lm_model) { results_robust = coeftest(lm_model, vcov = vcovHC(lm_model, &quot;HC1&quot;)) print(results_robust) } print_robust(lm_example) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.42988 1.58558 3.4246 0.0009108 *** ## ln_inv_gdp 1.42401 0.13196 10.7911 &lt; 2.2e-16 *** ## ln_ndg -1.98977 0.54524 -3.6493 0.0004297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now, unfortunately the coeftest function does not return an object that is easily transferred to a stargazer table. Thus, we will have to write another function. # needs sandwich compute_rob_se = function(lm_model) { vcov = vcovHC(lm_example, &quot;HC1&quot;) se = sqrt(diag(vcov)) } This makes our life somewhat easier. No, in order to compare the standard errors we could do the following. # run the model lm_example = lm(ln_gdp_85 ~ 1 + ln_inv_gdp + ln_ndg, data = mrw_non_oil) # obtain the robust ses rob_se = compute_rob_se(lm_example) stargazer(lm_example, lm_example, se = list(NULL, rob_se)) % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Mon, Sep 27, 2021 - 17:19:31 7.10 Some Graphs ggplot(mrw_non_oil) + geom_point(aes(x = ln_gdp_60, y = gdp_growth_60_85)) + labs(x = &quot;Log output per working-age adult in 1960&quot;, y = &quot;Growth rate: 1960-85&quot;, title = &quot;Unconditional Convergence&quot;) + theme_bw() Let’s try to get the residuals. lm_y = lm(gdp_growth_60_85 ~ 1+ ln_inv_gdp + ln_ndg + ln_school, data = mrw_non_oil) lm_x = lm(ln_gdp_60 ~ 1+ ln_inv_gdp + ln_ndg + ln_school, data = mrw_non_oil) y_res = lm_y$residuals x_res = lm_x$residuals graph_tibble = tibble( y = y_res, x = x_res) ggplot(graph_tibble) + geom_point(aes(x, y)) + labs(x = &quot;Res. X&quot;, y = &quot;Res. Y&quot;) + theme_bw() "],["linear-models-with-fixed-effects.html", "Chapter 8 Linear Models with Fixed Effects 8.1 Libraries 8.2 Review of Plotting: Recreating Figure 1 8.3 Review of Plotting: Recreating Figure 2 8.4 Loading the data for estimation 8.5 Pooled OLS with Time Effects 8.6 Fixed Effects with the lm function 8.7 Pooled OLS and FE with the lfe package 8.8 Review of IV 8.9 IV with felm", " Chapter 8 Linear Models with Fixed Effects 8.1 Libraries library(readxl) # read excel files library(tibble) # cuter dataframes library(dplyr) # data manipulation library(ggplot2) # graphs library(lfe) # fixed effects models library(stargazer) # nice tables library(ggrepel) # better graph labeling library(lmtest) # for coeftest function library(multiwayvcov) # (multiway) clustered standard errors library(AER) # instrumental variables library(ivpack) # robust standard errors for ivreg 8.2 Review of Plotting: Recreating Figure 1 # read data for first figure ajry_f1 = read_xls(&quot;data/ajry.xls&quot;, sheet = &quot;F1&quot;) %&gt;% rename(log_gdp_pc = lrgdpch, freedom_house = fhpolrigaug) ggplot(ajry_f1, aes(x = log_gdp_pc, y = freedom_house)) + geom_point(size = 0.5) + geom_text(aes(label = code), size = 2, hjust = 0, vjust = 0) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, size = 0.5, alpha = 0.2) + labs(x = &quot;Log GDP per Capita (1990-1999)&quot;, y = &quot;FH Measure of Democracy (1990-1999)&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; That is not bad, but we cannot read half of the label. Let’s try again, with the ggrepel package. ggplot(ajry_f1, aes(x = log_gdp_pc, y = freedom_house)) + geom_point(size = 0.5) + geom_text_repel(aes(label = code), size = 2) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;, size = 0.5, alpha = 0.2) + labs(x = &quot;Log GDP per Capita (1990-1999)&quot;, y = &quot;FH Measure of Democracy (1990-1999)&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: ggrepel: 24 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps rm(ajry_f1) 8.3 Review of Plotting: Recreating Figure 2 # read data for second figure ajry_f2 = read_xls(&quot;data/ajry.xls&quot;, sheet = &quot;F2&quot;) %&gt;% rename(freedom_house_change = s5fhpolrigaug, log_gdp_pc_change = s5lrgdpch) ggplot(ajry_f2, aes(x = log_gdp_pc_change, y = freedom_house_change)) + geom_point(size = 0.5) + geom_smooth(method = &quot;lm&quot;, size = 0.5, alpha = 0.2) + geom_text_repel(aes(label = code), size = 2) + labs(x = &quot;Change in GDP per Capita (1970-1995)&quot;, y = &quot;Change in FH Measure of Democracy (1970-1995)&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: ggrepel: 15 unlabeled data points (too many overlaps). Consider ## increasing max.overlaps 8.4 Loading the data for estimation ajry_df = read_xls(&quot;data/ajry.xls&quot;, sheet = 2) %&gt;% arrange(code_numeric, year_numeric) %&gt;% rename(log_gdp_pc = lrgdpch, freedom_house = fhpolrigaug) # generate lagged variables ajry_df = ajry_df %&gt;% group_by(code_numeric) %&gt;% mutate(lag_log_gdp_pc = lag(log_gdp_pc, order_by = year_numeric), lag_freedom_house = lag(freedom_house, order_by = year_numeric), lag2_nsave = lag(nsave, 2, order_by = year_numeric), lag_worldincome = lag(worldincome, order_by = year_numeric)) %&gt;% filter(sample == 1) 8.5 Pooled OLS with Time Effects # pooled ols with lm pooled_est = lm(freedom_house ~ -1 + lag_freedom_house + lag_log_gdp_pc + factor(year_numeric), data = ajry_df) # standard errors clustered by country vcov_country &lt;- cluster.vcov(pooled_est, ajry_df$code_numeric) coeftest(pooled_est, vcov_country) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## lag_freedom_house 0.7063698 0.0354523 19.9245 &lt; 2.2e-16 *** ## lag_log_gdp_pc 0.0723185 0.0099233 7.2878 6.705e-13 *** ## factor(year_numeric)33 -0.3468646 0.0617091 -5.6210 2.507e-08 *** ## factor(year_numeric)34 -0.4297435 0.0612981 -7.0107 4.543e-12 *** ## factor(year_numeric)35 -0.5462314 0.0669176 -8.1628 1.054e-15 *** ## factor(year_numeric)36 -0.4586181 0.0675307 -6.7913 1.976e-11 *** ## factor(year_numeric)37 -0.3969802 0.0689060 -5.7612 1.133e-08 *** ## factor(year_numeric)38 -0.4194864 0.0690369 -6.0763 1.789e-09 *** ## factor(year_numeric)39 -0.3994897 0.0650135 -6.1447 1.185e-09 *** ## factor(year_numeric)40 -0.3791493 0.0708188 -5.3538 1.084e-07 *** ## factor(year_numeric)41 -0.4031277 0.0653108 -6.1724 1.001e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.6 Fixed Effects with the lm function # pooled ols with lm fe_est = lm(freedom_house ~ -1 + lag_freedom_house + lag_log_gdp_pc + factor(year_numeric) + factor(code_numeric), data = ajry_df) # standard errors clustered by country vcov_country &lt;- cluster.vcov(fe_est, factor(ajry_df$code_numeric)) coeftest(fe_est, vcov_country) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## lag_freedom_house 3.7863e-01 5.0931e-02 7.4341 2.758e-13 *** ## lag_log_gdp_pc 1.0415e-02 3.4548e-02 0.3015 0.7631419 ## factor(year_numeric)33 -4.1689e-02 2.3873e-01 -0.1746 0.8614205 ## factor(year_numeric)34 -7.1530e-02 2.4413e-01 -0.2930 0.7695972 ## factor(year_numeric)35 -1.7531e-01 2.5152e-01 -0.6970 0.4859906 ## factor(year_numeric)36 -1.3071e-01 2.5226e-01 -0.5182 0.6044889 ## factor(year_numeric)37 -7.0236e-02 2.5313e-01 -0.2775 0.7814963 ## factor(year_numeric)38 -7.5191e-02 2.5788e-01 -0.2916 0.7706850 ## factor(year_numeric)39 -4.0343e-02 2.5574e-01 -0.1578 0.8746916 ## factor(year_numeric)40 6.2191e-05 2.6625e-01 0.0002 0.9998137 ## factor(year_numeric)41 2.8773e-03 2.6406e-01 0.0109 0.9913089 ## factor(code_numeric)4 1.6193e-01 2.9881e-02 5.4192 7.968e-08 *** ## factor(code_numeric)6 4.1576e-01 6.6405e-02 6.2610 6.292e-10 *** ## factor(code_numeric)7 2.2724e-01 2.0616e-02 11.0225 &lt; 2.2e-16 *** ## factor(code_numeric)8 2.1783e-01 7.0593e-02 3.0858 0.0021015 ** ## factor(code_numeric)9 5.8800e-01 9.3824e-02 6.2670 6.066e-10 *** ## factor(code_numeric)10 5.9152e-01 8.8594e-02 6.6767 4.620e-11 *** ## factor(code_numeric)11 2.0658e-02 1.2961e-02 1.5938 0.1113794 ## factor(code_numeric)14 8.2058e-03 2.8410e-02 0.2888 0.7727810 ## factor(code_numeric)15 5.8962e-01 8.9159e-02 6.6132 6.952e-11 *** ## factor(code_numeric)16 1.7273e-01 1.6391e-02 10.5381 &lt; 2.2e-16 *** ## factor(code_numeric)17 1.7885e-01 2.6830e-02 6.6660 4.952e-11 *** ## factor(code_numeric)18 3.4501e-01 2.2238e-02 15.5145 &lt; 2.2e-16 *** ## factor(code_numeric)19 4.2308e-01 5.5307e-02 7.6498 5.898e-14 *** ## factor(code_numeric)23 -5.2164e-02 3.7753e-02 -1.3817 0.1674502 ## factor(code_numeric)24 5.4440e-01 5.9107e-02 9.2103 &lt; 2.2e-16 *** ## factor(code_numeric)25 3.5215e-01 2.8596e-02 12.3148 &lt; 2.2e-16 *** ## factor(code_numeric)26 3.7473e-01 4.7202e-02 7.9388 7.051e-15 *** ## factor(code_numeric)27 5.7775e-01 7.8779e-02 7.3338 5.580e-13 *** ## factor(code_numeric)30 5.1360e-01 4.5905e-02 11.1884 &lt; 2.2e-16 *** ## factor(code_numeric)31 1.3606e-01 9.7552e-03 13.9473 &lt; 2.2e-16 *** ## factor(code_numeric)32 5.8759e-01 9.5013e-02 6.1843 1.003e-09 *** ## factor(code_numeric)33 5.8564e-01 1.0193e-01 5.7454 1.312e-08 *** ## factor(code_numeric)34 3.7967e-01 5.0385e-02 7.5354 1.343e-13 *** ## factor(code_numeric)35 2.4077e-02 1.2766e-02 1.8860 0.0596704 . ## factor(code_numeric)36 1.0009e-01 1.4282e-02 7.0081 5.205e-12 *** ## factor(code_numeric)37 6.4791e-02 1.3045e-02 4.9668 8.355e-07 *** ## factor(code_numeric)38 9.6525e-02 1.3631e-02 7.0813 3.176e-12 *** ## factor(code_numeric)39 4.1831e-01 4.5697e-02 9.1539 &lt; 2.2e-16 *** ## factor(code_numeric)40 1.5144e-01 1.6432e-02 9.2163 &lt; 2.2e-16 *** ## factor(code_numeric)41 3.3822e-01 1.8823e-02 17.9680 &lt; 2.2e-16 *** ## factor(code_numeric)42 5.9210e-01 5.8706e-02 10.0859 &lt; 2.2e-16 *** ## factor(code_numeric)43 -9.9454e-02 4.0976e-02 -2.4271 0.0154442 * ## factor(code_numeric)44 5.1843e-01 6.4375e-02 8.0533 2.986e-15 *** ## factor(code_numeric)45 5.1991e-01 7.8405e-02 6.6311 6.199e-11 *** ## factor(code_numeric)47 5.1661e-01 9.2338e-02 5.5948 3.052e-08 *** ## factor(code_numeric)51 5.2482e-01 5.3926e-02 9.7324 &lt; 2.2e-16 *** ## factor(code_numeric)52 5.8748e-01 9.5341e-02 6.1619 1.148e-09 *** ## factor(code_numeric)53 4.0917e-01 2.9577e-02 13.8340 &lt; 2.2e-16 *** ## factor(code_numeric)54 9.5593e-02 3.3655e-02 2.8404 0.0046223 ** ## factor(code_numeric)55 3.4636e-01 3.5097e-02 9.8687 &lt; 2.2e-16 *** ## factor(code_numeric)56 1.6095e-01 1.7164e-02 9.3770 &lt; 2.2e-16 *** ## factor(code_numeric)58 4.0409e-01 6.6964e-02 6.0345 2.457e-09 *** ## factor(code_numeric)59 5.8866e-01 5.8149e-02 10.1233 &lt; 2.2e-16 *** ## factor(code_numeric)60 7.5846e-02 5.0078e-02 1.5146 0.1302880 ## factor(code_numeric)61 7.6009e-02 3.1685e-02 2.3989 0.0166770 * ## factor(code_numeric)63 5.3987e-01 8.5772e-02 6.2943 5.133e-10 *** ## factor(code_numeric)64 2.6809e-01 4.5531e-02 5.8881 5.788e-09 *** ## factor(code_numeric)65 5.7704e-01 8.8222e-02 6.5408 1.103e-10 *** ## factor(code_numeric)66 1.4146e-01 5.2368e-02 2.7013 0.0070550 ** ## factor(code_numeric)67 5.8811e-01 8.9490e-02 6.5717 9.059e-11 *** ## factor(code_numeric)70 2.2235e-01 1.7367e-02 12.8028 &lt; 2.2e-16 *** ## factor(code_numeric)71 2.1795e-02 1.4917e-02 1.4611 0.1443897 ## factor(code_numeric)72 2.5364e-01 3.2729e-02 7.7499 2.847e-14 *** ## factor(code_numeric)73 2.0237e-01 3.8840e-02 5.2104 2.410e-07 *** ## factor(code_numeric)74 -2.0772e-02 7.9611e-03 -2.6092 0.0092480 ** ## factor(code_numeric)75 4.8964e-01 6.9596e-02 7.0354 4.330e-12 *** ## factor(code_numeric)76 5.7443e-01 4.4052e-02 13.0398 &lt; 2.2e-16 *** ## factor(code_numeric)77 3.4235e-01 3.4777e-02 9.8441 &lt; 2.2e-16 *** ## factor(code_numeric)78 3.1616e-01 2.8584e-02 11.0605 &lt; 2.2e-16 *** ## factor(code_numeric)79 3.4532e-01 2.2451e-02 15.3812 &lt; 2.2e-16 *** ## factor(code_numeric)80 5.4910e-01 4.7291e-02 11.6112 &lt; 2.2e-16 *** ## factor(code_numeric)81 1.2864e-01 1.9181e-02 6.7069 3.802e-11 *** ## factor(code_numeric)82 3.7392e-01 5.6692e-02 6.5957 7.775e-11 *** ## factor(code_numeric)83 1.9096e-01 9.2225e-03 20.7058 &lt; 2.2e-16 *** ## factor(code_numeric)84 4.9470e-01 3.8115e-02 12.9792 &lt; 2.2e-16 *** ## factor(code_numeric)85 5.8847e-01 7.6803e-02 7.6621 5.397e-14 *** ## factor(code_numeric)86 1.4631e-01 3.3904e-02 4.3155 1.796e-05 *** ## factor(code_numeric)88 5.8913e-01 9.0561e-02 6.5054 1.380e-10 *** ## factor(code_numeric)89 5.2999e-01 7.6612e-02 6.9179 9.518e-12 *** ## factor(code_numeric)90 5.8692e-01 8.6063e-02 6.8197 1.822e-11 *** ## factor(code_numeric)91 5.2817e-01 5.0711e-02 10.4152 &lt; 2.2e-16 *** ## factor(code_numeric)92 1.8031e-01 2.4815e-02 7.2661 8.941e-13 *** ## factor(code_numeric)93 5.6819e-01 8.2830e-02 6.8597 1.399e-11 *** ## factor(code_numeric)94 1.0524e-02 3.8074e-02 0.2764 0.7823069 ## factor(code_numeric)95 1.0774e-01 1.8093e-02 5.9548 3.926e-09 *** ## factor(code_numeric)96 -1.0717e-01 2.1725e-02 -4.9330 9.887e-07 *** ## factor(code_numeric)97 2.6984e-02 2.2666e-02 1.1905 0.2341998 ## factor(code_numeric)99 5.4006e-01 6.9106e-02 7.8149 1.767e-14 *** ## factor(code_numeric)101 3.3742e-01 4.0222e-02 8.3889 2.270e-16 *** ## factor(code_numeric)104 1.5938e-02 2.4818e-02 0.6422 0.5209229 ## factor(code_numeric)107 5.7775e-01 5.4977e-02 10.5088 &lt; 2.2e-16 *** ## factor(code_numeric)109 4.3280e-01 3.5299e-02 12.2611 &lt; 2.2e-16 *** ## factor(code_numeric)110 2.3431e-01 1.6531e-02 14.1739 &lt; 2.2e-16 *** ## factor(code_numeric)111 5.2749e-01 5.8978e-02 8.9440 &lt; 2.2e-16 *** ## factor(code_numeric)112 5.6232e-01 9.6567e-02 5.8232 8.417e-09 *** ## factor(code_numeric)113 5.9103e-01 5.2080e-02 11.3484 &lt; 2.2e-16 *** ## factor(code_numeric)114 2.2973e-01 2.3776e-02 9.6620 &lt; 2.2e-16 *** ## factor(code_numeric)115 5.6017e-01 2.0967e-02 26.7174 &lt; 2.2e-16 *** ## factor(code_numeric)116 3.1675e-01 2.6359e-02 12.0168 &lt; 2.2e-16 *** ## factor(code_numeric)118 3.3673e-01 5.3967e-02 6.2395 7.173e-10 *** ## factor(code_numeric)119 2.2015e-01 3.4675e-02 6.3490 3.663e-10 *** ## factor(code_numeric)120 1.8539e-01 2.3593e-02 7.8579 1.286e-14 *** ## factor(code_numeric)121 5.2026e-01 7.7427e-02 6.7194 3.506e-11 *** ## factor(code_numeric)125 2.0935e-01 2.0658e-02 10.1339 &lt; 2.2e-16 *** ## factor(code_numeric)126 6.7562e-02 8.9508e-03 7.5482 1.225e-13 *** ## factor(code_numeric)127 5.3418e-01 6.1703e-02 8.6573 &lt; 2.2e-16 *** ## factor(code_numeric)128 1.7093e-01 3.7344e-02 4.5771 5.479e-06 *** ## factor(code_numeric)129 3.3221e-01 4.5143e-02 7.3590 4.679e-13 *** ## factor(code_numeric)130 4.2931e-01 4.5681e-02 9.3980 &lt; 2.2e-16 *** ## factor(code_numeric)131 1.2193e-01 1.3065e-02 9.3325 &lt; 2.2e-16 *** ## factor(code_numeric)132 1.7334e-01 1.9743e-02 8.7801 &lt; 2.2e-16 *** ## factor(code_numeric)133 2.9120e-01 3.1504e-02 9.2434 &lt; 2.2e-16 *** ## factor(code_numeric)134 5.8899e-01 9.0980e-02 6.4738 1.683e-10 *** ## factor(code_numeric)135 5.8920e-01 9.0375e-02 6.5195 1.262e-10 *** ## factor(code_numeric)136 3.0599e-01 2.5966e-02 11.7840 &lt; 2.2e-16 *** ## factor(code_numeric)137 5.8882e-01 9.1452e-02 6.4386 2.099e-10 *** ## factor(code_numeric)140 1.7623e-01 1.8497e-02 9.5275 &lt; 2.2e-16 *** ## factor(code_numeric)141 4.8289e-01 2.9461e-02 16.3908 &lt; 2.2e-16 *** ## factor(code_numeric)142 3.0421e-01 3.9037e-02 7.7930 2.075e-14 *** ## factor(code_numeric)144 3.3488e-01 4.3930e-02 7.6230 7.156e-14 *** ## factor(code_numeric)145 3.8707e-01 3.4541e-02 11.2061 &lt; 2.2e-16 *** ## factor(code_numeric)147 4.8284e-01 4.3339e-02 11.1410 &lt; 2.2e-16 *** ## factor(code_numeric)148 4.8110e-01 5.1535e-02 9.3354 &lt; 2.2e-16 *** ## factor(code_numeric)150 4.3434e-01 5.9722e-02 7.2727 8.541e-13 *** ## factor(code_numeric)151 2.4963e-01 3.4267e-02 7.2846 7.862e-13 *** ## factor(code_numeric)153 1.5059e-01 1.9464e-02 7.7367 3.136e-14 *** ## factor(code_numeric)154 -1.4431e-02 5.2231e-02 -0.2763 0.7823904 ## factor(code_numeric)155 2.7024e-02 2.1621e-02 1.2499 0.2117055 ## factor(code_numeric)160 3.1820e-01 1.8678e-02 17.0366 &lt; 2.2e-16 *** ## factor(code_numeric)162 1.8732e-01 6.4579e-02 2.9007 0.0038276 ** ## factor(code_numeric)165 1.5966e-01 1.6637e-02 9.5967 &lt; 2.2e-16 *** ## factor(code_numeric)166 4.1406e-01 4.3556e-02 9.5064 &lt; 2.2e-16 *** ## factor(code_numeric)168 3.2435e-01 2.1140e-02 15.3429 &lt; 2.2e-16 *** ## factor(code_numeric)170 5.8626e-01 6.4695e-02 9.0620 &lt; 2.2e-16 *** ## factor(code_numeric)171 5.2006e-01 7.7996e-02 6.6677 4.897e-11 *** ## factor(code_numeric)172 5.7662e-01 9.2971e-02 6.2021 9.002e-10 *** ## factor(code_numeric)174 2.4048e-01 5.2931e-02 4.5433 6.411e-06 *** ## factor(code_numeric)175 3.7597e-02 1.6757e-02 2.2437 0.0251300 * ## factor(code_numeric)176 6.6391e-02 1.4184e-02 4.6806 3.369e-06 *** ## factor(code_numeric)177 6.3735e-02 1.3082e-02 4.8721 1.336e-06 *** ## factor(code_numeric)178 3.5007e-01 2.5720e-02 13.6110 &lt; 2.2e-16 *** ## factor(code_numeric)182 5.3754e-01 7.0228e-02 7.6543 5.710e-14 *** ## factor(code_numeric)183 9.1527e-02 3.1195e-02 2.9340 0.0034440 ** ## factor(code_numeric)184 3.5105e-01 4.4278e-02 7.9284 7.617e-15 *** ## factor(code_numeric)186 3.1766e-01 4.4714e-02 7.1043 2.715e-12 *** ## factor(code_numeric)187 1.4516e-01 3.9787e-02 3.6483 0.0002814 *** ## factor(code_numeric)188 9.2944e-02 3.5594e-02 2.6112 0.0091940 ** ## factor(code_numeric)189 1.5632e-01 4.1266e-02 3.7881 0.0001634 *** ## factor(code_numeric)191 4.5847e-01 6.3347e-02 7.2374 1.090e-12 *** ## factor(code_numeric)192 5.7786e-01 9.9262e-02 5.8216 8.493e-09 *** ## factor(code_numeric)194 -8.4544e-02 1.7685e-02 -4.7806 2.086e-06 *** ## factor(code_numeric)195 4.8397e-01 5.2186e-02 9.2739 &lt; 2.2e-16 *** ## factor(code_numeric)196 5.0183e-01 6.8964e-02 7.2767 8.304e-13 *** ## factor(code_numeric)197 -7.6516e-02 2.0959e-02 -3.6507 0.0002788 *** ## factor(code_numeric)203 1.6546e-01 2.9344e-02 5.6385 2.393e-08 *** ## factor(code_numeric)208 3.7521e-01 5.7867e-02 6.4840 1.578e-10 *** ## factor(code_numeric)209 3.3121e-02 2.7563e-02 1.2016 0.2298658 ## factor(code_numeric)210 1.9998e-01 2.0983e-02 9.5305 &lt; 2.2e-16 *** ## factor(code_numeric)211 1.7752e-01 2.0392e-02 8.7054 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.7 Pooled OLS and FE with the lfe package # pooled OLS felm1 = felm(freedom_house ~ lag_freedom_house + lag_log_gdp_pc | year_numeric | 0 | code_numeric, data = ajry_df) # FE felm2 = felm(freedom_house ~ lag_freedom_house + lag_log_gdp_pc | year_numeric + code_numeric | 0 | code_numeric, data = ajry_df) stargazer(felm1, felm2, type = &#39;text&#39;) ## ## ===================================================== ## Dependent variable: ## --------------------------------- ## freedom_house ## (1) (2) ## ----------------------------------------------------- ## lag_freedom_house 0.706*** 0.379*** ## (0.035) (0.046) ## ## lag_log_gdp_pc 0.072*** 0.010 ## (0.010) (0.032) ## ## ----------------------------------------------------- ## Observations 945 945 ## R2 0.725 0.796 ## Adjusted R2 0.722 0.755 ## Residual Std. Error 0.192 (df = 934) 0.180 (df = 785) ## ===================================================== ## Note: *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01 Notice that this command automatically spits out cluster-robust standard errors and passes them to stargazer. That is absolutely fantastic, right? For robust standard errors we have to work a bit. # function to recover robust standard errors get_felm_robust_se = function(felm_result) { felm_summary = summary(felm_result, robust = TRUE) robust_se = felm_summary$coefficients[, 2] } 8.8 Review of IV # Second Stage with ivreg, normal standard errors iv_sav = ivreg(freedom_house ~ lag_freedom_house + lag_log_gdp_pc + factor(year_numeric) + factor(code_numeric) | lag_freedom_house + lag2_nsave + factor(year_numeric) + factor(code_numeric), data = ajry_df) summary(iv_sav) ## ## Call: ## ivreg(formula = freedom_house ~ lag_freedom_house + lag_log_gdp_pc + ## factor(year_numeric) + factor(code_numeric) | lag_freedom_house + ## lag2_nsave + factor(year_numeric) + factor(code_numeric), ## data = ajry_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.677516 -0.076065 -0.002338 0.085441 0.599790 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.169672 0.488436 0.347 0.728406 ## lag_freedom_house 0.362910 0.035827 10.129 &lt; 2e-16 *** ## lag_log_gdp_pc -0.020493 0.070796 -0.289 0.772309 ## factor(year_numeric)34 -0.030477 0.038146 -0.799 0.424565 ## factor(year_numeric)35 -0.123590 0.041347 -2.989 0.002890 ** ## factor(year_numeric)36 -0.073594 0.047776 -1.540 0.123887 ## factor(year_numeric)37 -0.007184 0.053669 -0.134 0.893557 ## factor(year_numeric)38 -0.017182 0.058162 -0.295 0.767751 ## factor(year_numeric)39 0.021396 0.058867 0.363 0.716364 ## factor(year_numeric)40 0.064340 0.062995 1.021 0.307424 ## factor(year_numeric)41 0.069922 0.065121 1.074 0.283295 ## factor(code_numeric)6 0.479203 0.158838 3.017 0.002640 ** ## factor(code_numeric)8 0.284433 0.183283 1.552 0.121115 ## factor(code_numeric)9 0.675419 0.189699 3.560 0.000394 *** ## factor(code_numeric)10 0.673215 0.180377 3.732 0.000204 *** ## factor(code_numeric)14 -0.015853 0.120024 -0.132 0.894954 ## factor(code_numeric)15 0.672222 0.180646 3.721 0.000213 *** ## factor(code_numeric)16 0.179565 0.110810 1.620 0.105550 ## factor(code_numeric)17 0.179865 0.120176 1.497 0.134898 ## factor(code_numeric)18 0.336736 0.119985 2.806 0.005139 ** ## factor(code_numeric)23 -0.017314 0.211064 -0.082 0.934645 ## factor(code_numeric)24 0.591526 0.150815 3.922 9.58e-05 *** ## factor(code_numeric)25 0.377306 0.109020 3.461 0.000569 *** ## factor(code_numeric)26 0.417357 0.124468 3.353 0.000839 *** ## factor(code_numeric)27 0.646461 0.162333 3.982 7.49e-05 *** ## factor(code_numeric)30 0.545156 0.118985 4.582 5.40e-06 *** ## factor(code_numeric)31 0.145409 0.106096 1.371 0.170928 ## factor(code_numeric)32 0.676225 0.192015 3.522 0.000455 *** ## factor(code_numeric)33 0.681284 0.205619 3.313 0.000966 *** ## factor(code_numeric)34 0.392207 0.131573 2.981 0.002967 ** ## factor(code_numeric)35 0.015660 0.105347 0.149 0.881872 ## factor(code_numeric)36 0.091454 0.108214 0.845 0.398315 ## factor(code_numeric)37 0.042301 0.106907 0.396 0.692453 ## factor(code_numeric)38 0.079314 0.106817 0.743 0.458002 ## factor(code_numeric)39 0.456811 0.120229 3.800 0.000157 *** ## factor(code_numeric)40 0.161014 0.115192 1.398 0.162592 ## factor(code_numeric)41 0.350108 0.115632 3.028 0.002548 ** ## factor(code_numeric)42 0.640405 0.126330 5.069 5.04e-07 *** ## factor(code_numeric)43 -0.059135 0.171053 -0.346 0.729657 ## factor(code_numeric)44 0.576541 0.144363 3.994 7.15e-05 *** ## factor(code_numeric)45 0.591476 0.236263 2.503 0.012511 * ## factor(code_numeric)47 0.603525 0.220770 2.734 0.006410 ** ## factor(code_numeric)51 0.577474 0.152402 3.789 0.000163 *** ## factor(code_numeric)52 0.676447 0.192654 3.511 0.000473 *** ## factor(code_numeric)53 0.473624 0.109042 4.343 1.60e-05 *** ## factor(code_numeric)54 0.126940 0.125086 1.015 0.310521 ## factor(code_numeric)55 0.353559 0.113777 3.107 0.001958 ** ## factor(code_numeric)56 0.176670 0.104458 1.691 0.091196 . ## factor(code_numeric)58 0.468235 0.161766 2.895 0.003908 ** ## factor(code_numeric)61 0.051251 0.122568 0.418 0.675965 ## factor(code_numeric)63 0.619959 0.178506 3.473 0.000544 *** ## factor(code_numeric)64 0.306104 0.126789 2.414 0.016006 * ## factor(code_numeric)65 0.658951 0.180060 3.660 0.000270 *** ## factor(code_numeric)66 0.183726 0.150693 1.219 0.223152 ## factor(code_numeric)67 0.671077 0.181409 3.699 0.000232 *** ## factor(code_numeric)70 0.216119 0.106751 2.025 0.043273 * ## factor(code_numeric)71 0.034925 0.110002 0.317 0.750955 ## factor(code_numeric)72 0.252102 0.115699 2.179 0.029648 * ## factor(code_numeric)73 0.171593 0.139591 1.229 0.219364 ## factor(code_numeric)74 -0.014710 0.110618 -0.133 0.894247 ## factor(code_numeric)75 0.551922 0.159201 3.467 0.000557 *** ## factor(code_numeric)76 0.572273 0.144944 3.948 8.62e-05 *** ## factor(code_numeric)77 0.373408 0.113488 3.290 0.001048 ** ## factor(code_numeric)78 0.334633 0.112832 2.966 0.003116 ** ## factor(code_numeric)79 0.361195 0.103505 3.490 0.000512 *** ## factor(code_numeric)81 0.114460 0.116050 0.986 0.324307 ## factor(code_numeric)82 0.474590 0.156578 3.031 0.002521 ** ## factor(code_numeric)83 0.219953 0.106311 2.069 0.038894 * ## factor(code_numeric)84 0.499985 0.106987 4.673 3.52e-06 *** ## factor(code_numeric)85 0.657962 0.157570 4.176 3.32e-05 *** ## factor(code_numeric)86 0.173215 0.123747 1.400 0.162005 ## factor(code_numeric)88 0.673188 0.183360 3.671 0.000258 *** ## factor(code_numeric)89 0.600827 0.162467 3.698 0.000233 *** ## factor(code_numeric)90 0.666378 0.175112 3.805 0.000153 *** ## factor(code_numeric)91 0.565682 0.120193 4.706 3.00e-06 *** ## factor(code_numeric)92 0.212268 0.113589 1.869 0.062051 . ## factor(code_numeric)93 0.644689 0.170697 3.777 0.000171 *** ## factor(code_numeric)95 0.098251 0.110554 0.889 0.374440 ## factor(code_numeric)99 0.600061 0.165771 3.620 0.000315 *** ## factor(code_numeric)101 0.388717 0.124714 3.117 0.001898 ** ## factor(code_numeric)104 0.039985 0.208015 0.192 0.847620 ## factor(code_numeric)107 0.590006 0.148608 3.970 7.87e-05 *** ## factor(code_numeric)109 0.450721 0.104293 4.322 1.76e-05 *** ## factor(code_numeric)110 0.226397 0.113210 2.000 0.045884 * ## factor(code_numeric)112 0.652927 0.197087 3.313 0.000968 *** ## factor(code_numeric)113 0.636184 0.212508 2.994 0.002847 ** ## factor(code_numeric)114 0.250611 0.108748 2.304 0.021468 * ## factor(code_numeric)116 0.285630 0.112913 2.530 0.011622 * ## factor(code_numeric)118 0.387727 0.140368 2.762 0.005882 ** ## factor(code_numeric)120 0.168198 0.117269 1.434 0.151907 ## factor(code_numeric)125 0.194793 0.121680 1.601 0.109829 ## factor(code_numeric)126 0.029631 0.106110 0.279 0.780134 ## factor(code_numeric)127 0.587099 0.142284 4.126 4.10e-05 *** ## factor(code_numeric)128 0.141620 0.129898 1.090 0.275959 ## factor(code_numeric)129 0.372208 0.123189 3.021 0.002602 ** ## factor(code_numeric)130 0.465344 0.161116 2.888 0.003986 ** ## factor(code_numeric)131 0.138069 0.109420 1.262 0.207407 ## factor(code_numeric)132 0.166727 0.107667 1.549 0.121916 ## factor(code_numeric)133 0.320197 0.112626 2.843 0.004591 ** ## factor(code_numeric)134 0.673476 0.184172 3.657 0.000273 *** ## factor(code_numeric)135 0.673060 0.183000 3.678 0.000252 *** ## factor(code_numeric)136 0.318107 0.115146 2.763 0.005874 ** ## factor(code_numeric)137 0.673800 0.185087 3.640 0.000291 *** ## factor(code_numeric)140 0.253155 0.123030 2.058 0.039968 * ## factor(code_numeric)141 0.477834 0.203031 2.354 0.018856 * ## factor(code_numeric)142 0.341313 0.121621 2.806 0.005141 ** ## factor(code_numeric)144 0.375525 0.123661 3.037 0.002475 ** ## factor(code_numeric)145 0.413724 0.108475 3.814 0.000148 *** ## factor(code_numeric)147 0.512661 0.122242 4.194 3.07e-05 *** ## factor(code_numeric)148 0.530075 0.151003 3.510 0.000474 *** ## factor(code_numeric)150 0.490848 0.145648 3.370 0.000790 *** ## factor(code_numeric)151 0.290133 0.118858 2.441 0.014878 * ## factor(code_numeric)153 0.173212 0.114921 1.507 0.132176 ## factor(code_numeric)155 0.009336 0.114053 0.082 0.934784 ## factor(code_numeric)160 0.319616 0.110995 2.880 0.004096 ** ## factor(code_numeric)162 0.247927 0.162397 1.527 0.127265 ## factor(code_numeric)165 0.125936 0.112631 1.118 0.263873 ## factor(code_numeric)166 0.453163 0.120692 3.755 0.000187 *** ## factor(code_numeric)168 0.388228 0.126283 3.074 0.002187 ** ## factor(code_numeric)170 0.645564 0.225486 2.863 0.004314 ** ## factor(code_numeric)171 0.591186 0.235737 2.508 0.012359 * ## factor(code_numeric)172 0.664599 0.190596 3.487 0.000517 *** ## factor(code_numeric)174 0.291291 0.160047 1.820 0.069154 . ## factor(code_numeric)175 0.051675 0.111336 0.464 0.642687 ## factor(code_numeric)176 0.027898 0.109727 0.254 0.799373 ## factor(code_numeric)177 0.054677 0.108566 0.504 0.614668 ## factor(code_numeric)178 0.370369 0.105485 3.511 0.000473 *** ## factor(code_numeric)182 0.599432 0.151349 3.961 8.19e-05 *** ## factor(code_numeric)183 0.133137 0.125698 1.059 0.289860 ## factor(code_numeric)184 0.391082 0.121874 3.209 0.001389 ** ## factor(code_numeric)186 0.359565 0.135055 2.662 0.007926 ** ## factor(code_numeric)187 0.113755 0.132925 0.856 0.392392 ## factor(code_numeric)188 0.065677 0.127692 0.514 0.607167 ## factor(code_numeric)189 0.192374 0.207868 0.925 0.355023 ## factor(code_numeric)191 0.517016 0.145189 3.561 0.000393 *** ## factor(code_numeric)192 0.670964 0.201181 3.335 0.000895 *** ## factor(code_numeric)195 0.520239 0.149799 3.473 0.000544 *** ## factor(code_numeric)196 0.566123 0.154300 3.669 0.000261 *** ## factor(code_numeric)197 -0.089423 0.199570 -0.448 0.654225 ## factor(code_numeric)203 0.111615 0.207584 0.538 0.590955 ## factor(code_numeric)208 0.430353 0.145106 2.966 0.003115 ** ## factor(code_numeric)209 0.011614 0.116349 0.100 0.920514 ## factor(code_numeric)210 0.194566 0.110077 1.768 0.077545 . ## factor(code_numeric)211 0.194139 0.113144 1.716 0.086603 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1809 on 747 degrees of freedom ## Multiple R-Squared: 0.7938, Adjusted R-squared: 0.7543 ## Wald test: 20.12 on 143 and 747 DF, p-value: &lt; 2.2e-16 8.9 IV with felm # note the difference in the instrumental variable list. summary(felm(freedom_house ~ lag_freedom_house | year_numeric + code_numeric | (lag_log_gdp_pc ~ lag2_nsave) | code_numeric, data = ajry_df)) ## ## Call: ## felm(formula = freedom_house ~ lag_freedom_house | year_numeric + code_numeric | (lag_log_gdp_pc ~ lag2_nsave) | code_numeric, data = ajry_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.67752 -0.07606 -0.00234 0.08544 0.59979 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## lag_freedom_house 0.36291 0.05167 7.024 1.01e-10 *** ## `lag_log_gdp_pc(fit)` -0.02049 0.07463 -0.275 0.784 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1809 on 747 degrees of freedom ## (478 observations deleted due to missingness) ## Multiple R-squared(full model): 0.7938 Adjusted R-squared: 0.7543 ## Multiple R-squared(proj model): 0.1264 Adjusted R-squared: -0.04086 ## F-statistic(full model, *iid*):20.12 on 143 and 747 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 25.54 on 2 and 133 DF, p-value: 4.101e-10 ## F-statistic(endog. vars):0.07541 on 1 and 133 DF, p-value: 0.784 "],["instrumental-variables.html", "Chapter 9 Instrumental Variables 9.1 Libraries 9.2 Functions We Might Want to Have 9.3 A Brief Review of OLS: Replicating Table 2 9.4 A Brief Review of Making Figures: Replicating Figure 2 9.5 IV: Replicating Table 4 9.6 Adding Statistics to Stargazer", " Chapter 9 Instrumental Variables 9.1 Libraries library(AER) # ivreg command library(ivpack) # robust and clustered standard errors library(dplyr) # data manipulation library(ggplot2) # graphs library(tibble) # nice dataframes library(haven) # dta files library(stargazer)# tables library(sandwich) # robust se library(lmtest) # for print robust 9.2 Functions We Might Want to Have # function to compute robust standard errors for lm compute_lm_rse = function(lmodel) { vcov = vcovHC(lmodel, &quot;HC1&quot;) se = sqrt(diag(vcov)) } # function to easily pass lm results to stargazer lm_robust = function(lm_formula, data) { lm_result = lm(lm_formula, data) lm_se = compute_lm_rse(lm_result) return(list(&quot;result&quot; = lm_result, &quot;rse&quot; = lm_se)) } # summary of OLS with robust standard errors print_lm_robust = function(lmodel){ results_robust = coeftest(lmodel, vcov = vcovHC(lmodel, &quot;HC1&quot;)) print(results_robust) } 9.3 A Brief Review of OLS: Replicating Table 2 ajr_world = read_dta(&quot;data/ajr/maketable2/maketable2.dta&quot;) %&gt;% mutate(baseco = if_else(is.na(baseco), 0, 1)) %&gt;% mutate(africa = factor(africa), asia = factor(asia), other = factor(other)) ajr_base = ajr_world %&gt;% filter(baseco == 1) lm_1 = lm_robust(logpgp95 ~ avexpr, data = ajr_world) lm_2 = lm_robust(logpgp95 ~ avexpr, data = ajr_base) lm_3 = lm_robust(logpgp95 ~ avexpr + lat_abst, data = ajr_world) lm_4 = lm_robust(logpgp95 ~ avexpr + lat_abst, data = ajr_base) lm_5 = lm_robust(logpgp95 ~ avexpr + lat_abst + africa + asia + other, data = ajr_world) lm_6 = lm_robust(logpgp95 ~ avexpr + lat_abst + africa + asia + other, data = ajr_base) stargazer(lm_1$result, lm_2$result, lm_3$result, lm_4$result, lm_5$result, lm_6$result, type = &#39;latex&#39;, dep.var.labels = &quot;Log GDP per Capita in 1995&quot;, covariate.labels = c(&quot;Expropriation Risk&quot;, &quot;Latitude&quot;, &quot;Africa&quot;, &quot;Asia&quot;, &quot;Other&quot;, &quot;Constant&quot;), omit.stat = c(&#39;f&#39;, &#39;ser&#39;), se = list(lm_1$rse, lm_2$rse, lm_3$rse, lm_4$rse, lm_5$rse, lm_6$rse), title = &quot;OLS Results&quot;) % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Mon, Sep 27, 2021 - 17:19:45 9.4 A Brief Review of Making Figures: Replicating Figure 2 ggplot(ajr_base, aes(x = avexpr, y = logpgp95)) + geom_text(aes(label = shortnam), hjust = 0, vjust = 0, size = 3) + geom_smooth(method = &quot;lm&quot;, color = &quot;Black&quot;, size = 0.5) + labs(x = &quot;Average Expropriation Risk 1985-95&quot;, y = &quot;Log GDP per Capita, 1995&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; 9.5 IV: Replicating Table 4 9.5.1 Loading the data ajr_world = read_dta(&quot;data/ajr/maketable4/maketable4.dta&quot;) %&gt;% mutate(rich4 = factor(rich4)) ajr_base = ajr_world %&gt;% filter(baseco == 1) 9.5.2 Doing it by hand… # first stage iv_1_fs = lm(avexpr ~ logem4, ajr_base) print_lm_robust(iv_1_fs) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.34141 0.70429 13.2635 &lt; 2.2e-16 *** ## logem4 -0.60678 0.15020 -4.0399 0.0001499 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # add predicted variable to the dataset ajr_base = ajr_base %&gt;% mutate(avexpr_pred = iv_1_fs$fitted.values) # second stage iv_2_ss = lm(logpgp95 ~ avexpr_pred, ajr_base) print_lm_robust(iv_2_ss) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.90967 0.75692 2.5229 0.01422 * ## avexpr_pred 0.94428 0.12026 7.8522 7.142e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the robust standard errors are still not correct, because we did not account for the 2SLS standard error correction. The next command will do all of this for us. 9.5.3 The ivreg Command iv_1 = ivreg(logpgp95 ~ avexpr | logem4, data = ajr_base) summary(iv_1) ## ## Call: ## ivreg(formula = logpgp95 ~ avexpr | logem4, data = ajr_base) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.44903 -0.56242 0.07311 0.69564 1.71752 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.9097 1.0267 1.860 0.0676 . ## avexpr 0.9443 0.1565 6.033 9.8e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9483 on 62 degrees of freedom ## Multiple R-Squared: 0.187, Adjusted R-squared: 0.1739 ## Wald test: 36.39 on 1 and 62 DF, p-value: 9.799e-08 If we want to use robust or clustered standard errors, we can use the functionalities from the ivpack. # print robust standard errors robust.se(iv_1) ## [1] &quot;Robust Standard Errors&quot; ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.90967 1.17395 1.6267 0.1089 ## avexpr 0.94428 0.17610 5.3623 1.289e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Again, we can write a wrapper function that gives us both the model as well as the robust standard errors so that we can easily pass them to stargazer. Unfortunately, it is not perfect yet, because it also prints out some stuff. # function to easily pass robust iv results to stargazer iv_robust = function(iv_formula, data_iv) { iv_result = ivreg(formula = iv_formula, data = data_iv) iv_rse = robust.se(iv_result)[, 2] return(list(&quot;result&quot; = iv_result, &quot;rse&quot; = iv_rse)) } OK, now let’s replicate table 4. iv_1 = iv_robust(logpgp95 ~ avexpr | logem4, ajr_base) ## [1] &quot;Robust Standard Errors&quot; iv_2 = iv_robust(logpgp95 ~ avexpr + lat_abst | logem4 + lat_abst, ajr_base) ## [1] &quot;Robust Standard Errors&quot; ajr_noneo = ajr_base %&gt;% filter(!rich4 == 1) iv_3 = iv_robust(logpgp95 ~ avexpr | logem4, ajr_noneo) ## [1] &quot;Robust Standard Errors&quot; iv_4 = iv_robust(logpgp95 ~ avexpr + lat_abst | logem4 + lat_abst, ajr_noneo) ## [1] &quot;Robust Standard Errors&quot; stargazer(iv_1$result, iv_2$result, iv_3$result, iv_4$result, se = list(iv_1$rse, iv_2$rse, iv_3$rse, iv_4$rse), dep.var.labels = &quot;Log GDP in 1995&quot;, covariate.labels = c(&quot;Expropriation Risk&quot;, &quot;Latitude&quot;, &quot;Constant&quot;), omit.stat = &quot;ser&quot;, title = &quot;Instrumental Variable Results&quot;, type = &quot;latex&quot;) % Table created by stargazer v.5.2.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu % Date and time: Mon, Sep 27, 2021 - 17:19:46 9.6 Adding Statistics to Stargazer So far, we have not talked at all about additional test statistics. To look at those for our first specification, we can do this as follows: # generate value of robust f-test iv_1_fs = lm_robust(avexpr ~ logem4, ajr_base) iv_1_fs_null = lm_robust(avexpr ~ 1, ajr_base) f_iv_1 = waldtest(iv_1_fs$result, iv_1_fs_null$result, vcov = vcovHC(iv_1_fs$result, type=&quot;HC1&quot;))$F[2] ## Error in waldtest(iv_1_fs$result, iv_1_fs_null$result, vcov = vcovHC(iv_1_fs$result, : unused argument (vcov = vcovHC(iv_1_fs$result, type = &quot;HC1&quot;)) stargazer(iv_1_fs$result, se = list(iv_1_fs$rse), add.lines = list(c(&quot;Robust F&quot;, format(round(f_iv_1, 2), nsmall = 2))), dep.var.labels = &quot;Expropriation Risk&quot;, covariate.labels = c(&quot;Log of Settler Mortality&quot;, &quot;Constant&quot;), title = &quot;First Stage&quot;, omit.stat = c(&quot;ser&quot;, &quot;f&quot;), type = &quot;latex&quot;) ## Error in h(simpleError(msg, call)): error in evaluating the argument &#39;x&#39; in selecting a method for function &#39;format&#39;: object &#39;f_iv_1&#39; not found "],["difference-in-differences.html", "Chapter 10 Difference in Differences", " Chapter 10 Difference in Differences "],["regression-discontinuity-designs-rdd.html", "Chapter 11 Regression Discontinuity Designs (RDD) 11.1 Libraries 11.2 What is the paper about? 11.3 Let’s load the data. 11.4 Select and rename variables 11.5 Create Sample 11.6 Create Vector of Control Variables 11.7 Summary Statistics 11.8 Histogram of Islamic Win Margin 11.9 Doing RDD by Hand 11.10 rdrobust Package 11.11 RDD Plots", " Chapter 11 Regression Discontinuity Designs (RDD) 11.1 Libraries library(dplyr) # data manipulation library(tibble) # cuter dataframes library(haven) # read dta files library(rddensity) # density tests library(rdrobust) # rdd estimation library(ggplot2) # plot graphs library(stargazer) # nice tables library(stringr) # to combine strings 11.2 What is the paper about? Well, let’s look at the abstract: “Does Islamic political control affect women’s empowerment? Several countries have recently experienced Islamic parties coming to power through democratic elections. Due to strong support among religious conservatives, constituencies with Islamic rule often tend to exhibit poor women’s rights. Whether this reflects a causal relationship or a spurious one has so far gone unexplored. I provide the first piece of evidence using a new and unique data set of Turkish municipalities. In 1994, an Islamic party won multiple municipal mayor seats across the country. Using a regression discontinuity (RD) design, I compare municipalities where this Islamic party barely won or lost elections. Despite negative raw correlations, the RD results reveal that, over a period of six years, Islamic rule increased female secular high school education. Corresponding effects for men are systematically smaller and less precise. In the longer run, the effect on female education remained persistent up to 17 years after, and also reduced adolescent marriages. An analysis of long-run political effects of Islamic rule shows increased female political participation and an overall decrease in Islamic political preferences. The results are consistent with an explanation that emphasizes the Islamic party’s effectiveness in overcoming barriers to female entry for the poor and pious.” 11.3 Let’s load the data. empowerment = read_dta(&quot;data/meyersson/regdata0.dta&quot;) 11.4 Select and rename variables empowerment = empowerment %&gt;% select(vote_share_islam_1994 = vshr_islam1994, islamic_mayor_1994 = i94, log_pop_1994 = lpop1994, no_of_parties_1994 = partycount, share_women_hs_1520 = hischshr1520f, share_men_hs_1520 = hischshr1520m, pop_share_under_19 = ageshr19, pop_share_over_60 = ageshr60, sex_ratio_2000 = sexr, win_margin_islam_1994 = iwm94, household_size_2000 = shhs, district_center = merkezi, province_center = merkezp, metro_center = buyuk, sub_metro_center = subbuyuk, pd_1:pd_67, pcode = pcode) 11.5 Create Sample empowerment = empowerment %&gt;% filter(!is.na(share_women_hs_1520), !is.na(vote_share_islam_1994), !is.na(no_of_parties_1994), !is.na(win_margin_islam_1994), !is.na(islamic_mayor_1994), !is.na(share_men_hs_1520), !is.na(household_size_2000), !is.na(log_pop_1994), !is.na(pop_share_under_19), !is.na(pop_share_over_60), !is.na(sex_ratio_2000), !is.na(district_center), !is.na(province_center), !is.na(metro_center), !is.na(sub_metro_center)) 11.6 Create Vector of Control Variables Z = empowerment %&gt;% select(vote_share_islam_1994, no_of_parties_1994, household_size_2000, log_pop_1994, pop_share_under_19, pop_share_over_60, sex_ratio_2000, district_center, province_center, sub_metro_center, metro_center) 11.7 Summary Statistics 11.8 Histogram of Islamic Win Margin We first need to create a dataframe for which our outcome and explanatory variables are available. This is at least what Meyersson does. It is not so clear whether this is the best way of doing it. ggplot(empowerment) + geom_histogram(aes(x = win_margin_islam_1994, y = ..count../sum(..count..)*100), binwidth = 0.02, color = &quot;grey&quot;) + labs(x = &quot;Islamic Win Margin in 1994&quot;, y = &quot;Percent&quot;, title = &quot;Histogram of Islamic Win Margin&quot;) + theme_bw() out = rddensity(empowerment$win_margin_islam_1994) summary(out) ## ## Manipulation testing using local polynomial density estimation. ## ## Number of obs = 2629 ## Model = unrestricted ## Kernel = triangular ## BW method = estimated ## VCE method = jackknife ## ## c = 0 Left of c Right of c ## Number of obs 2314 315 ## Eff. Number of obs 965 301 ## Order est. (p) 2 2 ## Order bias (q) 3 3 ## BW est. (h) 0.305 0.283 ## ## Method T P &gt; |T| ## Robust -1.3937 0.1634 ## ## ## P-values of binomial tests (H0: p=0.5). ## ## Window Length / 2 &lt;c &gt;=c P&gt;|T| ## 0.004 11 9 0.8238 ## 0.009 18 26 0.2912 ## 0.013 32 34 0.9022 ## 0.017 42 48 0.5984 ## 0.021 52 57 0.7018 ## 0.026 68 62 0.6612 ## 0.030 74 71 0.8682 ## 0.034 94 79 0.2871 ## 0.038 113 86 0.0650 ## 0.043 131 90 0.0070 11.9 Doing RDD by Hand empower_left = empowerment %&gt;% filter(win_margin_islam_1994 &lt; 0, win_margin_islam_1994 &gt;= -.24) empower_right = empowerment %&gt;% filter(win_margin_islam_1994 &gt; 0, win_margin_islam_1994 &lt;= .24) lm_left = lm(share_women_hs_1520 ~ win_margin_islam_1994, empower_left) lm_right = lm(share_women_hs_1520 ~ win_margin_islam_1994, empower_right) intercept_left = lm_left$coefficients[1] intercept_right = lm_right$coefficients[1] difference = intercept_right - intercept_left print(str_c(&quot;The RD estimator is &quot;, difference, &quot;.&quot;)) ## [1] &quot;The RD estimator is 0.0317363445877732.&quot; 11.10 rdrobust Package 11.10.1 RDD with and without controls # rdd with and without controls: women summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 728 293 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.240 0.240 ## BW bias (b) 0.240 0.240 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.030 0.015 1.979 0.048 [0.000 , 0.059] ## Robust - - 1.411 0.158 [-0.011 , 0.066] ## ============================================================================= summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 728 293 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.240 0.240 ## BW bias (b) 0.240 0.240 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.025 0.009 2.894 0.004 [0.008 , 0.043] ## Robust - - 2.607 0.009 [0.008 , 0.053] ## ============================================================================= # rdd with and without controls: men summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.323, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 1038 304 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.323 0.323 ## BW bias (b) 0.323 0.323 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.012 0.010 1.159 0.247 [-0.008 , 0.031] ## Robust - - 1.607 0.108 [-0.004 , 0.045] ## ============================================================================= summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.323, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 1038 304 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.323 0.323 ## BW bias (b) 0.323 0.323 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.009 0.009 1.026 0.305 [-0.008 , 0.026] ## Robust - - 1.621 0.105 [-0.004 , 0.042] ## ============================================================================= 11.10.2 Different bandwidths # different bandwiths: women summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.120, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 369 220 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.120 0.120 ## BW bias (b) 0.120 0.120 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.030 0.011 2.757 0.006 [0.009 , 0.052] ## Robust - - 2.262 0.024 [0.005 , 0.064] ## ============================================================================= summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.480, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 1742 310 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.480 0.480 ## BW bias (b) 0.480 0.480 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.022 0.008 2.883 0.004 [0.007 , 0.036] ## Robust - - 2.677 0.007 [0.007 , 0.045] ## ============================================================================= # different bandwidths: men summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.161, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 493 254 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.161 0.161 ## BW bias (b) 0.161 0.161 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.018 0.011 1.686 0.092 [-0.003 , 0.040] ## Robust - - 1.238 0.216 [-0.011 , 0.048] ## ============================================================================= summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.646, covs = Z, cluster = empowerment$pcode)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 2270 314 ## Order est. (p) 1 1 ## Order bias (q) 2 2 ## BW est. (h) 0.646 0.646 ## BW bias (b) 0.646 0.646 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.008 0.008 1.061 0.289 [-0.007 , 0.024] ## Robust - - 1.011 0.312 [-0.009 , 0.029] ## ============================================================================= 11.10.3 Different control functions # different control functions: women summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, covs = Z, cluster = empowerment$pcode, p = 2)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 728 293 ## Order est. (p) 2 2 ## Order bias (q) 3 3 ## BW est. (h) 0.240 0.240 ## BW bias (b) 0.240 0.240 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.030 0.012 2.608 0.009 [0.008 , 0.053] ## Robust - - 2.569 0.010 [0.009 , 0.064] ## ============================================================================= summary(rdrobust(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, covs = Z, cluster = empowerment$pcode, p = 3)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 728 293 ## Order est. (p) 3 3 ## Order bias (q) 4 4 ## BW est. (h) 0.240 0.240 ## BW bias (b) 0.240 0.240 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.037 0.014 2.570 0.010 [0.009 , 0.065] ## Robust - - 1.889 0.059 [-0.001 , 0.070] ## ============================================================================= # different control functions: men summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.323, covs = Z, cluster = empowerment$pcode, p = 2)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 1038 304 ## Order est. (p) 2 2 ## Order bias (q) 3 3 ## BW est. (h) 0.323 0.323 ## BW bias (b) 0.323 0.323 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.019 0.012 1.626 0.104 [-0.004 , 0.042] ## Robust - - 1.557 0.120 [-0.006 , 0.052] ## ============================================================================= summary(rdrobust(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.323, covs = Z, cluster = empowerment$pcode, p = 3)) ## Call: rdrobust ## ## Number of Obs. 2629 ## BW type Manual ## Kernel Triangular ## VCE method NN ## ## Number of Obs. 2314 315 ## Eff. Number of Obs. 1038 304 ## Order est. (p) 3 3 ## Order bias (q) 4 4 ## BW est. (h) 0.323 0.323 ## BW bias (b) 0.323 0.323 ## rho (h/b) 1.000 1.000 ## Unique Obs. 2312 315 ## ## ============================================================================= ## Method Coef. Std. Err. z P&gt;|z| [ 95% C.I. ] ## ============================================================================= ## Conventional 0.023 0.015 1.557 0.120 [-0.006 , 0.052] ## Robust - - 1.268 0.205 [-0.012 , 0.055] ## ============================================================================= 11.11 RDD Plots 11.11.1 Main Outcomes: Women rdplot(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Women in Highschool (15-20)&quot;, y.lim = c(0, .5)) rdplot(empowerment$share_women_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Women in Highschool (15-20)&quot;, y.lim = c(0, .5)) empower_plot = empowerment %&gt;% filter(win_margin_islam_1994 &gt;= -0.240, win_margin_islam_1994 &lt;= 0.240) rdplot(empower_plot$share_women_hs_1520, empower_plot$win_margin_islam_1994, h = 0.240, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Women in Highschool (15-20)&quot;, y.lim = c(0, .25)) 11.11.2 Main Outcomes: Men rdplot(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Men in Highschool (15-20)&quot;, y.lim = c(0, .5)) rdplot(empowerment$share_men_hs_1520, empowerment$win_margin_islam_1994, h = 0.240, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Men in Highschool (15-20)&quot;, y.lim = c(0, .5)) rdplot(empower_plot$share_men_hs_1520, empower_plot$win_margin_islam_1994, h = 0.240, x.label = &quot;Running Variable: Islamic Vote Share&quot;, y.label = &quot;Share of Men in Highschool (15-20)&quot;, y.lim = c(0, .25)) "],["data-wrangling-with-dplyr.html", "Chapter 12 Data Wrangling with dplyr", " Chapter 12 Data Wrangling with dplyr The goal of this exercise is to give you some experience using functions from dplyr with a little less guidance. We want to build up some experience with is dplyr. Load it: library(dplyr) We are going to use a classic dataset from Graduate Econometrics texts that has gasoline consumption and some other variables by country for the 60’s and 70’s. The data is inside the package plm and called Gasoline. Load it as follows: # needs the plm libray ... install.packages(&quot;plm&quot;) library(plm) ## ## Attaching package: &#39;plm&#39; ## The following object is masked from &#39;package:lfe&#39;: ## ## sargan ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, lag, lead data(Gasoline, package = &quot;plm&quot;) Use the help to get a sense of what variable names mean in the data. Now work through the following exercises: Create a dataset gasoline which is a tibble Create a subset of that only has data from the 1960s. Do this two ways, once where you don’t use piping, %&gt;%, and once where you do. Create a subset that contains data from the years ranging from 1969 to 1973. Create a subset that contains data for the years 1969, 1973 and 1977. Create a dataset that contains only the columns country, year, lrpmg. Create a dataset that does not contain the columns country, year, lrpmg. Rename the column year to be called date. Select all columns that start with “l”. Select the columns country, year, and all columns that contain the letters “car”. What does the function pull() do? Try it on the column lrpmg. Create a grouped data set that groups the data by country. Ungroup the dataset from 10. Find the mean of lgaspcar by country. Call that variable avg_lgaspcar. Return a dataset that computes the mean of lgaspcar for france. Compute the mean, standard deviation, min and max of lgaspcar by country. Which country has the highest average gasoline consumption. Return a dataset that returns the countries with the highest and lowest average consumption. Add a variable count to the dataset that has the number of times each country appears in the data - Is it balanced? Create a meaningless dataset called spam that is the exponential of the sum of lgaspcar and lincomep. Also check out what happens if you replace mutate() with transmute(). Create the lead and lag of lgaspcar for each row of data. Call the new columns lead_lgaspcar and lag_lgaspcar. The following countries belong the to EU: eu_countries &lt;- c(&quot;austria&quot;, &quot;belgium&quot;, &quot;bulgaria&quot;, &quot;croatia&quot;, &quot;republic of cyprus&quot;, &quot;czech republic&quot;, &quot;denmark&quot;, &quot;estonia&quot;, &quot;finland&quot;, &quot;france&quot;, &quot;germany&quot;, &quot;greece&quot;, &quot;hungary&quot;, &quot;ireland&quot;, &quot;italy&quot;, &quot;latvia&quot;, &quot;lithuania&quot;, &quot;luxembourg&quot;, &quot;malta&quot;, &quot;netherla&quot;, &quot;poland&quot;, &quot;portugal&quot;, &quot;romania&quot;, &quot;slovakia&quot;, &quot;slovenia&quot;, &quot;spain&quot;, &quot;sweden&quot;, &quot;u.k.&quot;) Create a variable in_eu in the gasoline data which takes the value TRUE if a country is in the EU. (Note that the case of the string will matter!) Here’s a different way to classify countries: Mediterranean: france, italy, turkey, greece, spain Central Europe: germany, austria, switzerl, belgium, netherla Anglosphere: canada, u.s.a. , u.k., ireland Nordic: denmark, norway, sweden asia: japan Create a new variable region that uses these definitions. (Hint: case_when()) will likely be your friend here. Notice that in the country names switzerl and netherla are a little funky. Use the functions mutate and recode to replace the name with the full country name. Compute the variable quintile that computes which quintile of lgaspcar each country is. Do this only for 1960. Repeat this to create a variable decile with the appropriate definiiton. Create a variable high_consumption that takes the value TRUE if lgaspcar is higher than the yearly average for a given country. "],["plotting-with-ggplot.html", "Chapter 13 Plotting with ggplot 13.1 Load Data and Clean variable names 13.2 One Variable Graphs 13.3 Two Variable Graphs", " Chapter 13 Plotting with ggplot The goal of this set of exercises is to get more familarity with some of the common ggplot functions that we as economists use frequently. By the end of the exercise we should have a relatively nice looking figure that we would be happy to show someone. The data we will use comes from the UN Human Development Report from 2011. We are interested in understanding the relationship between the Human Development Index (HDI) and the Corruption Perceptions Index (CPI). First let us load the libraries we will need: library(readr) library(dplyr) library(ggplot2) Let’s get started! 13.1 Load Data and Clean variable names Load the data from the file data/EconomistData.csv. Convert all columns names to snakecase (i.e. my_variable) 13.2 One Variable Graphs First we work with some single variable plots. Create a histogram of the human development index. Customize the number of bins to make the plot look nicer than the default. Instead of a histogram, create a density plot of the HDI. Extend your plot by: In one graph plotting the densities by region. Creating separate plots per region, with the area under the density to be coloured blue. Repeat (1) and (2) for the corruption perception index. 13.3 Two Variable Graphs Now we are going to build up a ‘pretty’ graph that plots the corruption index (along the x-axis) against the human development index (along the y-axis). Create the simple scatter plot Let’s extend the plot in different ways. Modify the plot to (each point should be a different plot) Make the points blue Color the points by region Color the points by region and make the size of the point vary by HDI. Let’s extend the plot in (1) by adding some summary functions to it. Add a loess smoother Add a linear smoother, without the confidence interval. Color the line red. Add the line y ~ x + log(x), without the confidence interval. Color the line red. Now we will add the country names to the plot from (1). Use geom_text() to add country names to our plot. We might not want all the points labelled. Create the vector points_to_label &lt;- c(&quot;Russia&quot;, &quot;Venezuela&quot;, &quot;Iraq&quot;, &quot;Myanmar&quot;, &quot;Sudan&quot;, &quot;Afghanistan&quot;, &quot;Congo&quot;, &quot;Greece&quot;, &quot;Argentina&quot;, &quot;Brazil&quot;, &quot;India&quot;, &quot;Italy&quot;, &quot;China&quot;, &quot;South Africa&quot;, &quot;Spane&quot;, &quot;Botswana&quot;, &quot;Cape Verde&quot;, &quot;Bhutan&quot;, &quot;Rwanda&quot;, &quot;France&quot;, &quot;United States&quot;, &quot;Germany&quot;, &quot;Britain&quot;, &quot;Barbados&quot;, &quot;Norway&quot;, &quot;Japan&quot;, &quot;New Zealand&quot;, &quot;Singapore&quot;) Now adjust the code in (a) to only label these points Install the package ggrepel. Use the function geom_text_repel to repeat (b). Use the help to figure out how it works. Now let’s combine what we learned above, and from the class notes to build up a presentable notes. Proceed as follows: Create the simple scatter plot Make the points hollow, and colored by region. Adjust the size of the dots to make them easier to see. Add the line y ~ x + log(x), without the confidence interval. Color the line red. Change the color of the dots to be less ugly. I used scale_color_manual() but you don’t need to. Add meaningful x and y labels. And a title (which is centered). Can you add a note near the bottom of the figure to say that the data comes from “Transparency International and UN Human Development Report”? Label the points from points_to_label in 4b. Adjust the x and y axes to have a better range, and set of axis ticks. You are free to choose what you like. Move the legend to the bottom of the plot. Adjust the legend names so that they are easier to read and more meaningful. The easiest way to do this is to use dplyr to recode the region variable as a factor, and give it appropriate labels. Using the help menu for factor should help you here. "],["replicating-adh.html", "Chapter 14 Replicating ADH 14.1 Motivation 14.2 US Aggregate Data 14.3 Descriptive Stats in ADH using Weighted Statistics 14.4 Regression Analysis", " Chapter 14 Replicating ADH 14.1 Motivation TBD 14.2 US Aggregate Data We want to look for some aggregate facts about the US economy and its trade patterns to get a sense of how some macroeconomic indicators had evolved around China’s WTO accession. In particular we will show that Expansion of Chinese Trade. Essentially all of US trade growth since the 1990s is from the expansion of Chinese imports. Fall in Real Interest Rates Around the time the Chinese trade expanded. Expansion of the Trade Deficit during this time period. 14.2.1 Load Necessary Packages Install the ones you do not have yet. library(&quot;fredr&quot;) library(&quot;purrr&quot;) library(&quot;dplyr&quot;) library(&quot;readr&quot;) library(&quot;tidyr&quot;) library(&quot;ggplot2&quot;) library(&quot;magrittr&quot;) library(&quot;lubridate&quot;) library(&quot;PerformanceAnalytics&quot;) 14.2.2 Get Data from Federal Reserve We need the following variables from FRED: codes = c(&quot;GDP&quot;, &quot;IMP0015&quot;, &quot;IMPCH&quot;, &quot;EXP0015&quot;, &quot;GS1&quot;, &quot;CPILFESL&quot;) We also need to tell FRED our API key to authenticate ourselves. For this course, you can use the following command: api_key = &quot;YOUR KEY&quot; fredr_set_key(api_key) Use the fredr_series_observations command on a single variable to get the data for that variable Now, find the option to pull only data starting at 1990-01-01. Make sure you format this number as a date. What happens if you use the previous command on codes instead of a single variable name? Solve it by applying the correct map function which returns a dataframe by row-binding. Save your dataset as df_raw 14.2.3 Data Transformations For the rest of this part, take df_raw and save the transformed output as df: Now split your data into columns Rename your newly created columns as such: gdp &lt;- GDP, imp_ch &lt;- IMPCH, imp_all &lt;- IMP0015, exp_all &lt;- EXP0015, t_bill &lt;- GS1, cpi &lt;- CPILFESL cpi is coded in billions of USD while exports and imports are in millions, multiply cpi by 1000 to have all values in millions create additional variables for our date using the lubridate module. We want columns year, quarter, month, day that only contain this part from the date column. Find the correct functions in lubridate to achieve this Sort your data by date We see that gdp is coded quaterly while the imports and exports are per month. We need the data grouped annually and quarterly respectively for the next two parts: 14.2.4 Data Grouping Group df quarterly into a dataframe called df_quarter. In this dataset you want start_date as the minimum of date, gdp, imp_all, imp_ch, exp_all all aggregated as sums (how do you deal with NA’s?) cpi, t_bill aggregated as averages Group df annually into a dataframe called `df_year. In this dataset you want gdp, imp_all, imp_ch aggregated as sums (how do you deal with NA’s?) 14.2.5 Fact 1: Increase of Imports from China and the Rest of the World For this exercise, we are going to use df_year drop data from 2020 We want to create the following three variables: global_share = 100 * imp_all / gdp china_share = 100 * imp_ch / gdp nonchina_share = global_share - china_share Create the following graph years between 1991 and 2008 x-axis: years y-axis: china_share and nonchina_share (add to lines to your plot with different colors) a vertical line for x == 2008 14.2.6 Fact 2: Increasing trade-deficit of the US For this we will work with df_year again. drop data from 2020 We want to create a variable trade_deficit (imports - exports) And a variable trade_deficit_share for the share of the trade deficit compared to the gdp Plot trade_deficit_share over time Pick a nice color Add a vertical line to the plot for year == 2008 14.2.7 Fact 3: 400 basis point fall in real Interest rates leading into China Expansion For this we will work with df_quarter again. We need to calculate the inflation rate from the consumer price index (cpi) column find out how to calculate this look at the first lines of your dataset. What’s the problem? ungroup the dataset first before you repeat the previous step. look at the first lines of your dataset again. you now have the quaterly inflation rate Try to apply the Return.annualized function from the PerformanceAnalytics package to the newly created infl column inside mutate what’s the problem? We will solve this problem in two ways Use the function inside map_dbl instead (not in a mutate) assing the output to a new column of df_quarter called annum_return Use Vectorize(Return.annualized) inside mutate instead an save the output to annum_return2 What does the function do? Create a new variable called real_r as the difference between t_bill and one of your annum_return columns Plot real_r against date Add a vetical line at the date 2002-01-01 Hint: You have to parse the date first as a date and then convert it into a numerical value 14.3 Descriptive Stats in ADH using Weighted Statistics We are now going to work the the ‘micro’ data directly from ADH. Luckily, some of our coding friends at NYU Stern have done a tonne of the heavy lifting for us and merged all of ADH’s essential data together into one file. The trickiest thing to understand is the timing of the data and the variable names. Here is some info: 14.3.1 Details about timing is as follows. The start of the period is 1991 and then end is 2007. This is then divided into two periods. The first periods is 1991-2000, thus this is a 9 year time period. They convert stuff into a “comparable decadal scale” see Footnote 22. Thus, for values for this period, they multiply them by 10/9. The same issue arises for the second period which is 2000-2007. The values for this are again converted to “decadal scales” so they are multiplied by 10/7. The Appendix Table 2, reports the income variable and the decadal adjustments. In the summary statistics for the stuff that we care about, the ADH data is adjusted in this way described above. That is, variables starting with ‘l’ are in levels whereas variables starting with ‘d’ are the decadal equivalents. As necessary, we will tell you which variable to use, so that \\(*_somevariable\\) means to choose the appropriate level or decadal equivalent. We leave you to figure out which of the \\(l\\) or \\(d\\) variables to use. Do ask us if you are confused. 14.3.2 Understanding the Essence of the Paper and What Comes Next. Read Section 1 of ADH, so that you build an understanding of there main measure ‘IPW’ and what the paper is about. This will help you understand the context behind the remaining exercises in this notebook and those to follow. Your first task will be to compute some descriptive statistics from the data. To be more precise, you will replicate some of the key numbers in Appendix Table 2 of ADH. (On a side note, at least one of us thinks this table should be in the main text!) 14.3.3 Load Necessary Packages Install the ones you do not have yet. library(&quot;readr&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;Hmisc&quot;) 14.3.4 Load Data Like always, we are going to load the data and save it as a tibble df = read_csv(&quot;data/adh_data.csv&quot;) %&gt;% as_tibble() 14.3.5 Compute Simple Grouped Mean Find which years (yr) are reflected in the data. Compute the average number of chinese imports per worker ('l_tradeusch_pw &amp; d_tradeusch_pw) for each “year”. 14.3.6 Computed Weighted Group Means and Standard Deviations For the rest of the exercise, weight the mean by population count per region instead (l_popcount) and compare it with the numbers in the table. Repeat step 2 with weights. Now also compute the weighted standard deviations for both variables. Hint: Use the Hmisc package and find the relevant function. Now compute the mean and standard deviation of the average household wage and salary (l_avg_hhincwage_pc_pw, d_avg_hhincwage_pc_pw) And once more for share not in labor force (l_sh_nilf, d_sh_nilf) How well do your numbers line up with those reported in the paper? 14.4 Regression Analysis Let’s first load the necessary packages to read data and do fancy regressions: library(&quot;readr&quot;) library(&quot;tibble&quot;) library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) And let’s load the data like we always do: df = read_csv(&quot;data/adh_data.csv&quot;) 14.4.1 OLS regression The core of the paper is looking at what happened to laborer’s when theres an increase in us imports from china. Let’s try and replicate part of Table 9 - namely the estimate from panel A column 2. Their y variable is relchg_avg_hhincwage_pc_pw. The important x variable is decadal trade between the us and china d_tradeusch_pw. Run that simple regression Now add heteroskedasticity robust standard (HC1). Hint: Use the sandwich and lmtest packages Now we will start to add extra x variables. Start by adding t2 - a dummy variable for whether observation is in the second decade. Fit again with HC1 robust standard errors. 14.4.2 Clustering Let us now use clustertered standard errors instead. ADH cluster by statefip. Hint: use the felm package. Run the basic regression with clustering Add the following controls to your last regression: l_shind_manuf_cbp l_sh_popedu_c l_sh_popfborn l_sh_empl_f l_sh_routine33 l_task_outsource Add region fixed effects to your regression. First find all variables in the dataset that start with reg_ Add these to your last regression 14.4.3 Instrument Variables Instrument d_tradeusch_pw with d_tradeotch_pw_lag in your last regression Weight your regression by timepwt48 "],["dplyr-solution.html", "Chapter 15 dplyr solution", " Chapter 15 dplyr solution To be Added. "],["ggplot-solution.html", "Chapter 16 ggplot solution 16.1 Load Data and Clean variable names 16.2 One Variable Graphs 16.3 Two Variable Graphs", " Chapter 16 ggplot solution First let us load the libraries we will need: library(readr) library(dplyr) library(ggplot2) library(magrittr) ## ## Attaching package: &#39;magrittr&#39; ## The following object is masked from &#39;package:tidyr&#39;: ## ## extract ## The following object is masked from &#39;package:purrr&#39;: ## ## set_names Let’s get started! 16.1 Load Data and Clean variable names 16.1.1 1. Load the data from the file data/EconomistData.csv. df &lt;- read_csv(&quot;data/EconomistData.csv&quot;) ## Rows: 173 Columns: 5 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Country, Region ## dbl (3): HDI.Rank, HDI, CPI ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(df) ## Rows: 173 ## Columns: 5 ## $ Country &lt;chr&gt; &quot;Afghanistan&quot;, &quot;Albania&quot;, &quot;Algeria&quot;, &quot;Angola&quot;, &quot;Argentina&quot;, &quot;… ## $ HDI.Rank &lt;dbl&gt; 172, 70, 96, 148, 45, 86, 2, 19, 91, 53, 42, 146, 47, 65, 18,… ## $ HDI &lt;dbl&gt; 0.398, 0.739, 0.698, 0.486, 0.797, 0.716, 0.929, 0.885, 0.700… ## $ CPI &lt;dbl&gt; 1.5, 3.1, 2.9, 2.0, 3.0, 2.6, 8.8, 7.8, 2.4, 7.3, 5.1, 2.7, 7… ## $ Region &lt;chr&gt; &quot;Asia Pacific&quot;, &quot;East EU Cemt Asia&quot;, &quot;MENA&quot;, &quot;SSA&quot;, &quot;Americas… 16.1.2 2. Convert all columns names to snakecase (i.e. my_variable) library(janitor) ## ## Attaching package: &#39;janitor&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## chisq.test, fisher.test df %&lt;&gt;% clean_names(&quot;snake&quot;) 16.2 One Variable Graphs First we work with some single variable plots. 16.2.1 1. Create a histogram of the human development index. Customize the number of bins to make the plot look nicer than the default. df %&gt;% ggplot() + geom_histogram(aes(hdi), bins = 20 ) 16.2.2 2. Instead of a histogram, create a density plot of the HDI. Extend your plot by: 16.2.3 (a) In one graph plotting the densities by region. df %&gt;% ggplot(aes(cpi, fill = region)) + geom_density() 16.2.4 (b) Creating separate plots per region, with the area under the density to be coloured blue. df %&gt;% ggplot(aes(hdi)) + geom_density(fill = &quot;blue&quot;) + facet_wrap(vars(region), ncol = 2) 16.2.5 (c) Repeat (1) and (2) for the corruption perception index. df %&gt;% ggplot() + geom_histogram(aes(cpi), bins = 20 ) df %&gt;% ggplot(aes(cpi)) + geom_density(fill = &quot;blue&quot;) + facet_wrap(vars(region), ncol = 2) 16.3 Two Variable Graphs Now we are going to build up a ‘pretty’ graph that plots the corruption index (along the x-axis) against the human development index (along the y-axis). 16.3.1 1. Create the simple scatter plot df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point() 16.3.2 2. Let’s extend the plot in different ways. Modify the plot to (each point should be a different plot) 16.3.3 a. Make the points blue df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(color = &quot;blue&quot;) 16.3.4 b. Color the points by region df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region)) 16.3.5 c. Color the points by region and make the size of the point vary by HDI. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region, size = hdi )) 16.3.6 3. Let’s extend the plot in (1) by adding some summary functions to it. 16.3.7 a. Add a loess smoother df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; 16.3.8 b. Add a linear smoother, without the confidence interval. Color the line red. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point() + geom_smooth(se= FALSE, method = &quot;lm&quot;, color = &quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; 16.3.9 c. Add the line y ~ x + log(x), without the confidence interval. Color the line red. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region, size = hdi )) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) 16.3.10 4. Now we will add the country names to the plot from (1). For this we will need the package ggrepel because it makes this process easier. Install the package ggrepel. Use the function geom_text_repel library(ggrepel) 16.3.11 a. Use geom_text() to add country names to our plot. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + geom_text_repel(aes(label = country)) 16.3.12 b. We might not want all the points labelled. Create the vector points_to_label &lt;- c(&quot;Russia&quot;, &quot;Venezuela&quot;, &quot;Iraq&quot;, &quot;Myanmar&quot;, &quot;Sudan&quot;, &quot;Afghanistan&quot;, &quot;Congo&quot;, &quot;Greece&quot;, &quot;Argentina&quot;, &quot;Brazil&quot;, &quot;India&quot;, &quot;Italy&quot;, &quot;China&quot;, &quot;South Africa&quot;, &quot;Spane&quot;, &quot;Botswana&quot;, &quot;Cape Verde&quot;, &quot;Bhutan&quot;, &quot;Rwanda&quot;, &quot;France&quot;, &quot;United States&quot;, &quot;Germany&quot;, &quot;Britain&quot;, &quot;Barbados&quot;, &quot;Norway&quot;, &quot;Japan&quot;, &quot;New Zealand&quot;, &quot;Singapore&quot;) Now adjust the code in (a) to only label these points df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + geom_text_repel(aes(label = country), color = &quot;gray20&quot;, data = filter(df, country %in% points_to_label), force = 10) 16.3.13 5. Now let’s combine what we learned above, and from the class notes to build up a presentable notes. Proceed as follows: 16.3.14 a. Create the simple scatter plot df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point() 16.3.15 b. Make the points hollow, and colored by region. Adjust the size of the dots to make them easier to see. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) 16.3.16 c. Add the line y ~ x + log(x), without the confidence interval. Color the line red. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) 16.3.17 d. Change the color of the dots to be less ugly. I used scale_color_manual() but you don’t need to. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + scale_color_manual(name = &quot;&quot;, values = c(&quot;#24576D&quot;, &quot;#099DD7&quot;, &quot;#28AADC&quot;, &quot;#248E84&quot;, &quot;#F2583F&quot;, &quot;#96503F&quot;)) 16.3.18 e. Add meaningful x and y labels. And a title (which is centered). Can you add a note near the bottom of the figure to say that the data comes from “Transparency International and UN Human Development Report”? df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + scale_color_manual(name = &quot;&quot;, values = c(&quot;#24576D&quot;, &quot;#099DD7&quot;, &quot;#28AADC&quot;, &quot;#248E84&quot;, &quot;#F2583F&quot;, &quot;#96503F&quot;) ) + theme_bw() + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + xlab(&quot;Corruption Perceptions Index, 2011 (10=least corrupt)&quot;) + ylab(&quot;Human Development Index, 2011 (1=Best)&quot;) + ggtitle(&quot;Corruption and Human development&quot;, subtitle = waiver()) + labs(caption=&quot;Sources: Transparency International; UN Human Development Report&quot;) 16.3.19 f. Label the points from points_to_label in 4b. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + scale_color_manual(name = &quot;&quot;, values = c(&quot;#24576D&quot;, &quot;#099DD7&quot;, &quot;#28AADC&quot;, &quot;#248E84&quot;, &quot;#F2583F&quot;, &quot;#96503F&quot;) ) + geom_text_repel(aes(label = country), color = &quot;gray20&quot;, data = filter(df, country %in% points_to_label), force = 10) + theme_bw() + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + xlab(&quot;Corruption Perceptions Index, 2011 (10=least corrupt)&quot;) + ylab(&quot;Human Development Index, 2011 (1=Best)&quot;) + ggtitle(&quot;Corruption and Human development&quot;, subtitle = waiver()) + labs(caption=&quot;Sources: Transparency International; UN Human Development Report&quot;) 16.3.20 g. Adjust the x and y axes to have a better range, and set of axis ticks. You are free to choose what you like. df %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + scale_color_manual(name = &quot;&quot;, values = c(&quot;#24576D&quot;, &quot;#099DD7&quot;, &quot;#28AADC&quot;, &quot;#248E84&quot;, &quot;#F2583F&quot;, &quot;#96503F&quot;) ) + geom_text_repel(aes(label = country), color = &quot;gray20&quot;, data = filter(df, country %in% points_to_label), force = 10) + scale_x_continuous( limits = c(.9, 10.5), breaks = 1:10) + scale_y_continuous( limits = c(0.2, 1.0), breaks = seq(0.2, 1.0, by = 0.1) ) + theme_bw() + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + xlab(&quot;Corruption Perceptions Index, 2011 (10=least corrupt)&quot;) + ylab(&quot;Human Development Index, 2011 (1=Best)&quot;) + ggtitle(&quot;Corruption and Human development&quot;, subtitle = waiver()) + labs(caption=&quot;Sources: Transparency International; UN Human Development Report&quot;) 16.3.21 h. Move the legend to the bottom of the plot. Adjust the legend names so that they are easier to read and more meaningful. The easiest way to do this is to use dplyr to recode the region variable as a factor, and give it appropriate labels. Using the help menu for factor should help you here. df2 &lt;- df %&gt;% mutate(region = factor(region, levels = c(&quot;EU W. Europe&quot;, &quot;Americas&quot;, &quot;Asia Pacific&quot;, &quot;East EU Cemt Asia&quot;, &quot;MENA&quot;, &quot;SSA&quot;), labels = c(&quot;OECD&quot;, &quot;Americas&quot;, &quot;Asia &amp;\\nOceania&quot;, &quot;Central &amp;\\nEastern Europe&quot;, &quot;Middle East &amp;\\nnorth Africa&quot;, &quot;Sub-Saharan\\nAfrica&quot;) ) ) df2 %&gt;% ggplot(aes(x = cpi, y = hdi)) + geom_point(aes(color = region), shape = 1, size = 2.5, stroke = 1.25) + geom_smooth(se= FALSE, method = &quot;lm&quot;, formula = y ~ x + log(x), color = &quot;red&quot;) + geom_text_repel(aes(label = country), color = &quot;gray20&quot;, data = filter(df, country %in% points_to_label), force = 10) + scale_color_manual(name = &quot;&quot;, values = c(&quot;#24576D&quot;, &quot;#099DD7&quot;, &quot;#28AADC&quot;, &quot;#248E84&quot;, &quot;#F2583F&quot;, &quot;#96503F&quot;)) + theme_bw() + theme(legend.position = &quot;bottom&quot;, plot.title = element_text(hjust = 0.5)) + xlab(&quot;Corruption Perceptions Index, 2011 (10=least corrupt)&quot;) + ylab(&quot;Human Development Index, 2011 (1=Best)&quot;) + ggtitle(&quot;Corruption and Human development&quot;, subtitle = waiver()) + scale_x_continuous( limits = c(.9, 10.5), breaks = 1:10) + scale_y_continuous( limits = c(0.2, 1.0), breaks = seq(0.2, 1.0, by = 0.1) ) + labs(caption=&quot;Sources: Transparency International; UN Human Development Report&quot;) "],["replicating-adh-1.html", "Chapter 17 Replicating ADH 17.1 Aggregate Facts 17.2 Descriptive Stats 17.3 Regression", " Chapter 17 Replicating ADH 17.1 Aggregate Facts TBD 17.2 Descriptive Stats Install the ones you do not have yet. library(&quot;readr&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;Hmisc&quot;) ## Loading required package: lattice ## Loading required package: Formula ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## src, summarize ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units 17.2.1 Load Data Like always, we are going to load the data and save it as a tibble df = read_csv(&quot;data/adh_data.csv&quot;) %&gt;% as_tibble ## Rows: 1444 Columns: 208 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): city ## dbl (207): czone, statefip, yr, t2, timepwt48, reg_midatl, reg_encen, reg_wn... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 17.2.2 Compute Simple Grouped Mean Find which years (yr) are reflected in the data. unique(df$yr) ## [1] 1990 2000 Compute the average number of chinese imports per worker (l_tradeusch_pw &amp; d_tradeusch_pw) for each “year”. df_yr = group_by(df, yr) df_yr %&gt;% summarise(l_tradeusch_pw_avg = mean(l_tradeusch_pw), d_tradeusch_pw_avg = mean(d_tradeusch_pw) ) ## # A tibble: 2 × 3 ## yr l_tradeusch_pw_avg d_tradeusch_pw_avg ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 0.364 1.18 ## 2 2000 1.12 2.64 17.2.3 Computed Weighted Group Means and Standard Deviations For the rest of the exercise, weight the mean by population count per region instead (l_popcount) and compare it with the numbers in the table. Repeat step 2 with weights. df_yr %&gt;% summarise(l_tradeusch_pw = weighted.mean(l_tradeusch_pw), d_tradeusch_pw = weighted.mean(d_tradeusch_pw)) ## # A tibble: 2 × 3 ## yr l_tradeusch_pw d_tradeusch_pw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 0.364 1.18 ## 2 2000 1.12 2.64 Now also compute the weighted standard deviations for both variables. Hint: Use the Hmisc package and find the relevant function. df_yr %&gt;% summarise(l_tradeusch_pw_avg = weighted.mean(l_tradeusch_pw), d_tradeusch_pw_avg = weighted.mean(d_tradeusch_pw), l_tradeusch_pw_sd = sqrt(wtd.var(l_tradeusch_pw, l_popcount)), d_tradeusch_pw_sd = sqrt(wtd.var(d_tradeusch_pw, l_popcount)) ) ## # A tibble: 2 × 5 ## yr l_tradeusch_pw_avg d_tradeusch_pw_avg l_tradeusch_pw_sd d_tradeusch_pw_… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 0.364 1.18 0.325 0.992 ## 2 2000 1.12 2.64 0.897 2.01 Now compute the mean and standard deviation of the average household wage and salary (l_avg_hhincwage_pc_pw, d_avg_hhincwage_pc_pw) df_yr %&gt;% summarise(l_avg_hhincwage_pc_pw_avg = weighted.mean(l_avg_hhincwage_pc_pw), d_avg_hhincwage_pc_pw_avg = weighted.mean(d_avg_hhincwage_pc_pw), l_avg_hhincwage_pc_pw_sd = sqrt(wtd.var(l_avg_hhincwage_pc_pw, l_popcount)), d_avg_hhincwage_pc_pw_sd = sqrt(wtd.var(d_avg_hhincwage_pc_pw, l_popcount)) ) ## # A tibble: 2 × 5 ## yr l_avg_hhincwage_p… d_avg_hhincwage_… l_avg_hhincwage_… d_avg_hhincwage_… ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 18093. 3618. 4697. 1568. ## 2 2000 21711. 1077. 5445. 2621. And once more for share not in labor force (l_sh_nilf, d_sh_nilf) df_yr %&gt;% summarise(l_sh_nilf_avg = weighted.mean(l_sh_nilf, , l_popcount), d_sh_nilf_avg = weighted.mean(d_sh_nilf, , l_popcount), l_sh_nilf_sd = sqrt(wtd.var(l_sh_nilf, l_popcount)), d_sh_nilf_sd = sqrt(wtd.var(d_sh_nilf, l_popcount)) ) ## # A tibble: 2 × 5 ## yr l_sh_nilf_avg d_sh_nilf_avg l_sh_nilf_sd d_sh_nilf_sd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1990 26.7 -0.344 4.33 2.55 ## 2 2000 26.4 -1.51 4.39 2.56 17.3 Regression Let’s first load the necessary packages to read data and do fancy regressions: library(&quot;readr&quot;) library(&quot;tibble&quot;) library(&quot;dplyr&quot;) library(&quot;sandwich&quot;) library(&quot;lmtest&quot;) library(&quot;lfe&quot;) And let’s load the data like we always do: df = read_csv(&quot;data/adh_data.csv&quot;) ## Rows: 1444 Columns: 208 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): city ## dbl (207): czone, statefip, yr, t2, timepwt48, reg_midatl, reg_encen, reg_wn... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 17.3.1 OLS regression The core of the paper is looking at what happened to laborer’s when theres an increase in us imports from china. Let’s try and replicate part of Table 9 - namely the estimate from panel A column 2. Their y variable is relchg_avg_hhincwage_pc_pw. The important x variable is decadal trade between the us and china d_tradeusch_pw. Run that simple regression lm_1 = lm(relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw, data = df) summary(lm_1) ## ## Call: ## lm(formula = relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.789 -8.411 -0.663 7.715 49.684 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.0720 0.3889 41.33 &lt;2e-16 *** ## d_tradeusch_pw -1.6466 0.1212 -13.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 11.89 on 1442 degrees of freedom ## Multiple R-squared: 0.1135, Adjusted R-squared: 0.1129 ## F-statistic: 184.6 on 1 and 1442 DF, p-value: &lt; 2.2e-16 Now add heteroskedasticity robust standard (HC1). Hint: Use the sandwich and lmtest packages coeftest(lm_1, vcov = vcovHC(lm_1, type=&quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 16.07198 0.57211 28.0923 &lt; 2.2e-16 *** ## d_tradeusch_pw -1.64663 0.28496 -5.7785 9.219e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now we will start to add extra x variables. Start by adding t2 - a dummy variable for whether observation is in the second decade. Fit again with HC1 robust standard errors. lm_2 = lm(relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2, data = df) coeftest(lm_2, vcov = vcovHC(lm_2, type=&quot;HC1&quot;)) ## ## t test of coefficients: ## ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.59096 0.39889 54.1271 &lt; 2.2e-16 *** ## d_tradeusch_pw -0.88316 0.16804 -5.2555 1.698e-07 *** ## t2 -13.94769 0.57869 -24.1023 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 17.3.2 Clustering Let us now use clustertered standard errors instead. ADH cluster by statefip. Hint: use the felm command from the lfe package Run the basic regression with clustering felm_1 = felm(relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 | 0 | 0 | statefip, data = df) summary(felm_1) ## ## Call: ## felm(formula = relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 | 0 | 0 | statefip, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.707 -6.259 -0.799 5.024 43.128 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## (Intercept) 21.5910 0.9841 21.941 &lt; 2e-16 *** ## d_tradeusch_pw -0.8832 0.2544 -3.472 0.000532 *** ## t2 -13.9477 1.8146 -7.686 2.79e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.831 on 1441 degrees of freedom ## Multiple R-squared(full model): 0.3944 Adjusted R-squared: 0.3936 ## Multiple R-squared(proj model): 0.3944 Adjusted R-squared: 0.3936 ## F-statistic(full model, *iid*):469.3 on 2 and 1441 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 68.5 on 2 and 47 DF, p-value: 1.179e-14 Add the following controls to your last regression: l_shind_manuf_cbp l_sh_popedu_c l_sh_popfborn l_sh_empl_f l_sh_routine33 l_task_outsource felm_2 = felm(relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource | 0 | 0 | statefip, data = df) summary(felm_2) ## ## Call: ## felm(formula = relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource | 0 | 0 | statefip, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -37.329 -5.869 -0.534 5.229 40.819 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## (Intercept) 43.38967 8.70748 4.983 7.02e-07 *** ## d_tradeusch_pw -0.37033 0.13283 -2.788 0.00537 ** ## t2 -13.79402 1.76456 -7.817 1.04e-14 *** ## l_shind_manuf_cbp -0.18211 0.03835 -4.749 2.25e-06 *** ## l_sh_popedu_c -0.13418 0.05494 -2.442 0.01471 * ## l_sh_popfborn -0.13180 0.08678 -1.519 0.12903 ## l_sh_empl_f 0.31270 0.07987 3.915 9.46e-05 *** ## l_sh_routine33 -1.04448 0.23945 -4.362 1.38e-05 *** ## l_task_outsource 4.21857 1.72489 2.446 0.01458 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.218 on 1435 degrees of freedom ## Multiple R-squared(full model): 0.4699 Adjusted R-squared: 0.4669 ## Multiple R-squared(proj model): 0.4699 Adjusted R-squared: 0.4669 ## F-statistic(full model, *iid*): 159 on 8 and 1435 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 28.75 on 8 and 47 DF, p-value: 1.276e-15 Add region fixed effects to your regression. First find all variables in the dataset that start with reg_ Add these to your last regression names(select(df, starts_with(&quot;reg_&quot;))) ## [1] &quot;reg_midatl&quot; &quot;reg_encen&quot; &quot;reg_wncen&quot; &quot;reg_satl&quot; &quot;reg_escen&quot; ## [6] &quot;reg_wscen&quot; &quot;reg_mount&quot; &quot;reg_pacif&quot; felm_3 = felm(relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | 0 | statefip, data = df) summary(felm_3) ## ## Call: ## felm(formula = relchg_avg_hhincwage_pc_pw ~ d_tradeusch_pw + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | 0 | statefip, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.256 -5.408 -0.643 4.993 40.596 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## (Intercept) 37.81934 10.64339 3.553 0.000393 *** ## d_tradeusch_pw -0.41327 0.12965 -3.188 0.001465 ** ## t2 -13.67579 1.73663 -7.875 6.70e-15 *** ## l_shind_manuf_cbp -0.15404 0.03266 -4.716 2.64e-06 *** ## l_sh_popedu_c -0.09418 0.05679 -1.658 0.097449 . ## l_sh_popfborn -0.09372 0.07883 -1.189 0.234674 ## l_sh_empl_f 0.18554 0.09601 1.933 0.053492 . ## l_sh_routine33 -0.79294 0.26449 -2.998 0.002764 ** ## l_task_outsource 4.26729 1.87809 2.272 0.023226 * ## reg_midatl 2.06339 1.97681 1.044 0.296757 ## reg_encen 2.10566 2.13354 0.987 0.323844 ## reg_wncen 6.91844 2.14751 3.222 0.001303 ** ## reg_satl 0.87864 1.89366 0.464 0.642723 ## reg_escen 4.30400 2.16206 1.991 0.046705 * ## reg_wscen 5.05509 2.07366 2.438 0.014900 * ## reg_mount 4.48765 2.04204 2.198 0.028136 * ## reg_pacif -0.17986 1.91287 -0.094 0.925100 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.997 on 1427 degrees of freedom ## Multiple R-squared(full model): 0.4977 Adjusted R-squared: 0.4921 ## Multiple R-squared(proj model): 0.4977 Adjusted R-squared: 0.4921 ## F-statistic(full model, *iid*):88.38 on 16 and 1427 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 21.68 on 16 and 47 DF, p-value: &lt; 2.2e-16 17.3.3 Instrument Variables Instrument d_tradeusch_pw with d_tradeotch_pw_lag in your last regression felm_4 = felm(relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, data = df) summary(felm_4) ## ## Call: ## felm(formula = relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -36.066 -5.524 -0.555 4.996 42.042 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## (Intercept) 35.87405 10.64255 3.371 0.000769 *** ## t2 -12.28244 1.97304 -6.225 6.31e-10 *** ## l_shind_manuf_cbp -0.08113 0.04139 -1.960 0.050177 . ## l_sh_popedu_c -0.08755 0.05778 -1.515 0.129955 ## l_sh_popfborn -0.08900 0.08150 -1.092 0.275021 ## l_sh_empl_f 0.18856 0.09896 1.905 0.056935 . ## l_sh_routine33 -0.75086 0.26165 -2.870 0.004168 ** ## l_task_outsource 4.20832 1.86424 2.257 0.024135 * ## reg_midatl 1.97306 1.93354 1.020 0.307693 ## reg_encen 1.62608 2.22012 0.732 0.464027 ## reg_wncen 6.46027 2.19861 2.938 0.003353 ** ## reg_satl 0.41119 1.94593 0.211 0.832677 ## reg_escen 5.02788 2.28463 2.201 0.027914 * ## reg_wscen 4.62044 2.12783 2.171 0.030063 * ## reg_mount 3.89093 2.09533 1.857 0.063523 . ## reg_pacif -0.94648 2.00055 -0.473 0.636206 ## `d_tradeusch_pw(fit)` -1.27575 0.43200 -2.953 0.003197 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.186 on 1427 degrees of freedom ## Multiple R-squared(full model): 0.4765 Adjusted R-squared: 0.4706 ## Multiple R-squared(proj model): 0.4765 Adjusted R-squared: 0.4706 ## F-statistic(full model, *iid*):86.45 on 16 and 1427 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 18.74 on 16 and 47 DF, p-value: 3.486e-15 ## F-statistic(endog. vars):8.721 on 1 and 47 DF, p-value: 0.004898 Weight your regression by timepwt48 The felm function is a bit picky on the order of the weights. Let us first try to define weights at the end after the data argument like so: felm_5 = felm(relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, data = df, weights = timepwt48) ## Error in eval(mf[[wpos]], pf): object &#39;timepwt48&#39; not found summary(felm_5) ## Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;summary&#39;: object &#39;felm_5&#39; not found Felm didn’t find timepwt48 because it only assumes that columns are in df before you define data = df. We can solve this in two ways. A good rule is to have data = df as the last argument. felm_5 = felm(relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, weights = timepwt48, data = df) ## Error in eval(mf[[wpos]], pf): object &#39;timepwt48&#39; not found summary(felm_5) ## Error in h(simpleError(msg, call)): error in evaluating the argument &#39;object&#39; in selecting a method for function &#39;summary&#39;: object &#39;felm_5&#39; not found Alternatively, you can define weights after data = df, but then you have to define the weights as df$timepwft48 like so: felm_5 = felm(relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, data = df, weights = df$timepwt48) summary(felm_5) ## ## Call: ## felm(formula = relchg_avg_hhincwage_pc_pw ~ 1 + t2 + l_shind_manuf_cbp + l_sh_popedu_c + l_sh_popfborn + l_sh_empl_f + l_sh_routine33 + l_task_outsource + reg_midatl + reg_encen + reg_wncen + reg_satl + reg_escen + reg_wscen + reg_mount + reg_pacif | 0 | (d_tradeusch_pw ~ d_tradeotch_pw_lag) | statefip, data = df, weights = df$timepwt48) ## ## Weighted Residuals: ## Min 1Q Median 3Q Max ## -3.2404 -0.1084 -0.0033 0.1114 2.8633 ## ## Coefficients: ## Estimate Cluster s.e. t value Pr(&gt;|t|) ## (Intercept) 60.67937 9.14264 6.637 4.54e-11 *** ## t2 -9.05462 2.65665 -3.408 0.000672 *** ## l_shind_manuf_cbp 0.06791 0.08505 0.798 0.424746 ## l_sh_popedu_c 0.10292 0.11146 0.923 0.355954 ## l_sh_popfborn 0.07652 0.08050 0.950 0.342020 ## l_sh_empl_f -0.21015 0.16952 -1.240 0.215297 ## l_sh_routine33 -1.01014 0.22888 -4.413 1.09e-05 *** ## l_task_outsource 5.56661 1.48578 3.747 0.000186 *** ## reg_midatl -0.56489 1.55722 -0.363 0.716840 ## reg_encen -2.61723 1.97713 -1.324 0.185797 ## reg_wncen 1.93904 1.63154 1.188 0.234846 ## reg_satl -2.73867 1.49871 -1.827 0.067855 . ## reg_escen 0.60288 1.53128 0.394 0.693856 ## reg_wscen -1.68621 1.73888 -0.970 0.332354 ## reg_mount -2.36081 1.40184 -1.684 0.092385 . ## reg_pacif -6.27918 2.02160 -3.106 0.001933 ** ## `d_tradeusch_pw(fit)` -2.14156 0.59462 -3.602 0.000327 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2791 on 1427 degrees of freedom ## Multiple R-squared(full model): 0.4278 Adjusted R-squared: 0.4214 ## Multiple R-squared(proj model): 0.4278 Adjusted R-squared: 0.4214 ## F-statistic(full model, *iid*):77.51 on 16 and 1427 DF, p-value: &lt; 2.2e-16 ## F-statistic(proj model): 44.53 on 16 and 47 DF, p-value: &lt; 2.2e-16 ## F-statistic(endog. vars):12.97 on 1 and 47 DF, p-value: 0.0007602 And now we have the numbers reported in Column 2 of Panel A of Table 9 of the paper. "],["references.html", "References", " References "]]
